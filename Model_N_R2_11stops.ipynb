{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "b3ff328c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3ff328c",
        "outputId": "d69052af-bfa4-4fdb-840e-cb04167f123e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: calplot in /usr/local/lib/python3.10/dist-packages (0.1.7.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from calplot) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from calplot) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->calplot) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install calplot pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "619525a5",
      "metadata": {
        "id": "619525a5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import calendar\n",
        "import calplot # actually used\n",
        "\n",
        "# Score model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "04a64600",
      "metadata": {
        "id": "04a64600"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/main/ProjectAssignmentData/Dataset-PT.csv\"\n",
        "df = pd.read_csv(url,header=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6b091cb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "6b091cb5",
        "outputId": "8174c5fd-06d3-4010-c3ee-0aba13b69fa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Calendar_date  route_id  bus_id  stop_sequence  arrival_delay  dwell_time  \\\n",
              "0       20220108         4   41344              1            151           0   \n",
              "1       20220108         4   41344              2            185          24   \n",
              "2       20220108         4   41344              3            186           0   \n",
              "3       20220108         4   41344              4            202          12   \n",
              "4       20220108         4   41344              5            242          21   \n",
              "\n",
              "   travel_time_for_previous_section  scheduled_travel_time  \\\n",
              "0                                 0                    120   \n",
              "1                               171                     45   \n",
              "2                                55                     41   \n",
              "3                                42                     94   \n",
              "4                                98                     86   \n",
              "\n",
              "   upstream_stop_delay  origin_delay  ...  factor(weather)Rain  \\\n",
              "0                  100           100  ...                    0   \n",
              "1                  151           100  ...                    0   \n",
              "2                  185           100  ...                    0   \n",
              "3                  186           100  ...                    0   \n",
              "4                  202           100  ...                    0   \n",
              "\n",
              "   factor(weather)Snow  factor(temperature)Cold  \\\n",
              "0                    0                        0   \n",
              "1                    0                        0   \n",
              "2                    0                        0   \n",
              "3                    0                        0   \n",
              "4                    0                        0   \n",
              "\n",
              "   factor(temperature)Extra_cold factor(temperature)Normal  \\\n",
              "0                              0                         1   \n",
              "1                              0                         1   \n",
              "2                              0                         1   \n",
              "3                              0                         1   \n",
              "4                              0                         1   \n",
              "\n",
              "  factor(day_of_week)weekday factor(day_of_week)weekend  \\\n",
              "0                          0                          1   \n",
              "1                          0                          1   \n",
              "2                          0                          1   \n",
              "3                          0                          1   \n",
              "4                          0                          1   \n",
              "\n",
              "  factor(time_of_day)Afternoon_peak  factor(time_of_day)Morning_peak  \\\n",
              "0                                 0                                0   \n",
              "1                                 0                                0   \n",
              "2                                 0                                0   \n",
              "3                                 0                                0   \n",
              "4                                 0                                0   \n",
              "\n",
              "   factor(time_of_day)Off-peak  \n",
              "0                            1  \n",
              "1                            1  \n",
              "2                            1  \n",
              "3                            1  \n",
              "4                            1  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9203afd-4199-474c-819c-7d1151be4e0a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Calendar_date</th>\n",
              "      <th>route_id</th>\n",
              "      <th>bus_id</th>\n",
              "      <th>stop_sequence</th>\n",
              "      <th>arrival_delay</th>\n",
              "      <th>dwell_time</th>\n",
              "      <th>travel_time_for_previous_section</th>\n",
              "      <th>scheduled_travel_time</th>\n",
              "      <th>upstream_stop_delay</th>\n",
              "      <th>origin_delay</th>\n",
              "      <th>...</th>\n",
              "      <th>factor(weather)Rain</th>\n",
              "      <th>factor(weather)Snow</th>\n",
              "      <th>factor(temperature)Cold</th>\n",
              "      <th>factor(temperature)Extra_cold</th>\n",
              "      <th>factor(temperature)Normal</th>\n",
              "      <th>factor(day_of_week)weekday</th>\n",
              "      <th>factor(day_of_week)weekend</th>\n",
              "      <th>factor(time_of_day)Afternoon_peak</th>\n",
              "      <th>factor(time_of_day)Morning_peak</th>\n",
              "      <th>factor(time_of_day)Off-peak</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20220108</td>\n",
              "      <td>4</td>\n",
              "      <td>41344</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20220108</td>\n",
              "      <td>4</td>\n",
              "      <td>41344</td>\n",
              "      <td>2</td>\n",
              "      <td>185</td>\n",
              "      <td>24</td>\n",
              "      <td>171</td>\n",
              "      <td>45</td>\n",
              "      <td>151</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20220108</td>\n",
              "      <td>4</td>\n",
              "      <td>41344</td>\n",
              "      <td>3</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>41</td>\n",
              "      <td>185</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20220108</td>\n",
              "      <td>4</td>\n",
              "      <td>41344</td>\n",
              "      <td>4</td>\n",
              "      <td>202</td>\n",
              "      <td>12</td>\n",
              "      <td>42</td>\n",
              "      <td>94</td>\n",
              "      <td>186</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20220108</td>\n",
              "      <td>4</td>\n",
              "      <td>41344</td>\n",
              "      <td>5</td>\n",
              "      <td>242</td>\n",
              "      <td>21</td>\n",
              "      <td>98</td>\n",
              "      <td>86</td>\n",
              "      <td>202</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9203afd-4199-474c-819c-7d1151be4e0a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9203afd-4199-474c-819c-7d1151be4e0a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9203afd-4199-474c-819c-7d1151be4e0a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef419b85-e79b-4f70-8d01-42a31cb2bfed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef419b85-e79b-4f70-8d01-42a31cb2bfed')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef419b85-e79b-4f70-8d01-42a31cb2bfed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df.head(5)\n",
        "# df.size 545104 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a76c1d26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76c1d26",
        "outputId": "59f49601-ae58-4221-a1d5-5fce758208bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of outliers removed: 228\n",
            "Size of the original DataFrame: 545103\n",
            "Size of the DataFrame after removing outliers: 544875\n"
          ]
        }
      ],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate z-scores for the 'arrival_delay' column\n",
        "z_scores = stats.zscore(df['arrival_delay'])\n",
        "\n",
        "# Get boolean array indicating the location of outliers\n",
        "outliers = (z_scores > 7) | (z_scores < -7)\n",
        "\n",
        "# Count the number of outliers\n",
        "num_outliers = outliers.sum()\n",
        "\n",
        "# Print the number of outliers\n",
        "print(f\"Number of outliers removed: {num_outliers}\")\n",
        "\n",
        "# Remove the outliers\n",
        "df_no_outliers = df[~outliers]\n",
        "\n",
        "# Verify the new size of the DataFrame\n",
        "print(f\"Size of the original DataFrame: {len(df)}\")\n",
        "print(f\"Size of the DataFrame after removing outliers: {len(df_no_outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "121b1747",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "121b1747",
        "outputId": "28fa5cb8-5382-4b92-b86f-9d6abaeef6da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(201801, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "desired_stop_sequences = [0, 1, 5, 8, 12, 14, 16, 19, 20, 21, 23]  # Replace with your specific stop sequence number\n",
        "\n",
        "# Create a boolean mask for rows with the desired stop sequence\n",
        "mask = df_no_outliers['stop_sequence'].isin(desired_stop_sequences)\n",
        "\n",
        "# Filter the DataFrame to keep only the rows with the desired stop sequence\n",
        "df_selected_stops = df_no_outliers[mask]\n",
        "\n",
        "df_selected_stops.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qmvHv5Dx6K4N",
      "metadata": {
        "id": "qmvHv5Dx6K4N"
      },
      "source": [
        "# Model with weather"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KXw6UThKUGJg",
      "metadata": {
        "id": "KXw6UThKUGJg"
      },
      "source": [
        "### Neural Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "60985502",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60985502",
        "outputId": "bf40ecc2-b049-477f-d491-593921b7be0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "311/311 [==============================] - 2s 3ms/step - loss: 49.3944 - mae: 49.3944 - val_loss: 27.9025 - val_mae: 27.9025\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.6930 - mae: 27.6930 - val_loss: 27.1943 - val_mae: 27.1943\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.5284 - mae: 27.5284 - val_loss: 27.1096 - val_mae: 27.1096\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4787 - mae: 27.4787 - val_loss: 27.1770 - val_mae: 27.1770\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.5419 - mae: 27.5419 - val_loss: 27.0801 - val_mae: 27.0801\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4600 - mae: 27.4600 - val_loss: 27.0994 - val_mae: 27.0994\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.5041 - mae: 27.5041 - val_loss: 27.0993 - val_mae: 27.0993\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4815 - mae: 27.4815 - val_loss: 27.2533 - val_mae: 27.2533\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4664 - mae: 27.4664 - val_loss: 27.3111 - val_mae: 27.3111\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5143 - mae: 27.5143 - val_loss: 27.2470 - val_mae: 27.2470\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4591 - mae: 27.4591 - val_loss: 27.1380 - val_mae: 27.1380\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4452 - mae: 27.4452 - val_loss: 27.0044 - val_mae: 27.0044\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 27.5148 - mae: 27.5148 - val_loss: 27.2436 - val_mae: 27.2436\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4570 - mae: 27.4570 - val_loss: 27.2125 - val_mae: 27.2125\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 5ms/step - loss: 27.5397 - mae: 27.5397 - val_loss: 27.0619 - val_mae: 27.0619\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 27.4719 - mae: 27.4719 - val_loss: 27.1401 - val_mae: 27.1401\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4295 - mae: 27.4295 - val_loss: 27.1640 - val_mae: 27.1640\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 2s 5ms/step - loss: 27.4393 - mae: 27.4393 - val_loss: 27.0715 - val_mae: 27.0715\n",
            "Epoch 19/100\n",
            "311/311 [==============================] - 2s 5ms/step - loss: 27.4770 - mae: 27.4770 - val_loss: 27.3366 - val_mae: 27.3366\n",
            "Epoch 20/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 27.4256 - mae: 27.4256 - val_loss: 27.1247 - val_mae: 27.1247\n",
            "Epoch 21/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.4953 - mae: 27.4953 - val_loss: 27.0618 - val_mae: 27.0618\n",
            "Epoch 22/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 27.4552 - mae: 27.4552 - val_loss: 27.3199 - val_mae: 27.3199\n",
            "96/96 [==============================] - 1s 4ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 2s 4ms/step - loss: 118.1465 - mae: 118.1465 - val_loss: 112.5064 - val_mae: 112.5064\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 97.7226 - mae: 97.7226 - val_loss: 72.7337 - val_mae: 72.7337\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 40.3670 - mae: 40.3670 - val_loss: 26.9540 - val_mae: 26.9540\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.6381 - mae: 26.6381 - val_loss: 26.5227 - val_mae: 26.5227\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.5221 - mae: 26.5221 - val_loss: 26.7637 - val_mae: 26.7637\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.3795 - mae: 26.3795 - val_loss: 26.3640 - val_mae: 26.3640\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.3077 - mae: 26.3077 - val_loss: 26.5736 - val_mae: 26.5736\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0952 - mae: 26.0952 - val_loss: 25.9180 - val_mae: 25.9180\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1740 - mae: 26.1740 - val_loss: 25.9494 - val_mae: 25.9494\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0895 - mae: 26.0895 - val_loss: 25.7042 - val_mae: 25.7042\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.9739 - mae: 25.9739 - val_loss: 25.6625 - val_mae: 25.6625\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.9871 - mae: 25.9871 - val_loss: 25.8784 - val_mae: 25.8784\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0765 - mae: 26.0765 - val_loss: 25.7834 - val_mae: 25.7834\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0673 - mae: 26.0673 - val_loss: 25.8375 - val_mae: 25.8375\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0688 - mae: 26.0688 - val_loss: 25.7889 - val_mae: 25.7889\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.9840 - mae: 25.9840 - val_loss: 25.9803 - val_mae: 25.9803\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0688 - mae: 26.0688 - val_loss: 25.7757 - val_mae: 25.7757\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.9063 - mae: 25.9063 - val_loss: 25.7991 - val_mae: 25.7991\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.8417 - mae: 25.8417 - val_loss: 25.7354 - val_mae: 25.7354\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0021 - mae: 26.0021 - val_loss: 25.6358 - val_mae: 25.6358\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0274 - mae: 26.0274 - val_loss: 25.9285 - val_mae: 25.9285\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0602 - mae: 26.0602 - val_loss: 26.0084 - val_mae: 26.0084\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0238 - mae: 26.0238 - val_loss: 25.9230 - val_mae: 25.9230\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.9768 - mae: 25.9768 - val_loss: 25.7043 - val_mae: 25.7043\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0694 - mae: 26.0694 - val_loss: 25.9936 - val_mae: 25.9936\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0341 - mae: 26.0341 - val_loss: 25.6953 - val_mae: 25.6953\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 25.9867 - mae: 25.9867 - val_loss: 25.8120 - val_mae: 25.8120\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 25.9606 - mae: 25.9606 - val_loss: 25.6804 - val_mae: 25.6804\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.1737 - mae: 26.1737 - val_loss: 25.6355 - val_mae: 25.6355\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 25.9646 - mae: 25.9646 - val_loss: 25.8840 - val_mae: 25.8840\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 26.0559 - mae: 26.0559 - val_loss: 25.7191 - val_mae: 25.7191\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 26.0051 - mae: 26.0051 - val_loss: 26.2122 - val_mae: 26.2122\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 25.9537 - mae: 25.9537 - val_loss: 25.7860 - val_mae: 25.7860\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 25.9767 - mae: 25.9767 - val_loss: 25.7665 - val_mae: 25.7665\n",
            "Epoch 35/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0200 - mae: 26.0200 - val_loss: 26.2160 - val_mae: 26.2160\n",
            "Epoch 36/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 26.0738 - mae: 26.0738 - val_loss: 25.7981 - val_mae: 25.7981\n",
            "Epoch 37/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0122 - mae: 26.0122 - val_loss: 25.9240 - val_mae: 25.9240\n",
            "Epoch 38/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0036 - mae: 26.0036 - val_loss: 25.7700 - val_mae: 25.7700\n",
            "Epoch 39/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 25.9561 - mae: 25.9561 - val_loss: 25.6575 - val_mae: 25.6575\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 69.4143 - mae: 69.4143 - val_loss: 20.9072 - val_mae: 20.9072\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 16.2083 - mae: 16.2083 - val_loss: 14.7291 - val_mae: 14.7291\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4032 - mae: 14.4032 - val_loss: 14.6651 - val_mae: 14.6651\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4829 - mae: 14.4829 - val_loss: 14.8051 - val_mae: 14.8051\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4408 - mae: 14.4408 - val_loss: 14.6672 - val_mae: 14.6672\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3308 - mae: 14.3308 - val_loss: 14.6927 - val_mae: 14.6927\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4004 - mae: 14.4004 - val_loss: 14.5389 - val_mae: 14.5389\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4160 - mae: 14.4160 - val_loss: 14.6069 - val_mae: 14.6069\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3953 - mae: 14.3953 - val_loss: 14.9534 - val_mae: 14.9534\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4007 - mae: 14.4007 - val_loss: 14.8493 - val_mae: 14.8493\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4240 - mae: 14.4240 - val_loss: 14.6545 - val_mae: 14.6545\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4071 - mae: 14.4071 - val_loss: 14.8145 - val_mae: 14.8145\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4099 - mae: 14.4099 - val_loss: 14.6465 - val_mae: 14.6465\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4443 - mae: 14.4443 - val_loss: 14.8632 - val_mae: 14.8632\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3848 - mae: 14.3848 - val_loss: 14.8024 - val_mae: 14.8024\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3794 - mae: 14.3794 - val_loss: 14.7504 - val_mae: 14.7504\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.4120 - mae: 14.4120 - val_loss: 14.6751 - val_mae: 14.6751\n",
            "96/96 [==============================] - 0s 927us/step\n",
            "Epoch 1/100\n",
            "93/93 [==============================] - 1s 3ms/step - loss: 155.4011 - mae: 155.4011 - val_loss: 151.2153 - val_mae: 151.2153\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 142.8013 - mae: 142.8013 - val_loss: 121.6800 - val_mae: 121.6800\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 69.8245 - mae: 69.8245 - val_loss: 22.1957 - val_mae: 22.1957\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 17.3422 - mae: 17.3422 - val_loss: 15.2433 - val_mae: 15.2433\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 13.7733 - mae: 13.7733 - val_loss: 13.0457 - val_mae: 13.0457\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 12.3722 - mae: 12.3722 - val_loss: 12.2019 - val_mae: 12.2019\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.8986 - mae: 11.8986 - val_loss: 11.4041 - val_mae: 11.4041\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6046 - mae: 11.6046 - val_loss: 11.3998 - val_mae: 11.3998\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6836 - mae: 11.6836 - val_loss: 11.7253 - val_mae: 11.7253\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6267 - mae: 11.6267 - val_loss: 11.3620 - val_mae: 11.3620\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6997 - mae: 11.6997 - val_loss: 11.7056 - val_mae: 11.7056\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5631 - mae: 11.5631 - val_loss: 11.4343 - val_mae: 11.4343\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6866 - mae: 11.6866 - val_loss: 11.8042 - val_mae: 11.8042\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6370 - mae: 11.6370 - val_loss: 12.0548 - val_mae: 12.0548\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6380 - mae: 11.6380 - val_loss: 11.3150 - val_mae: 11.3150\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5482 - mae: 11.5482 - val_loss: 11.5835 - val_mae: 11.5835\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5911 - mae: 11.5911 - val_loss: 11.8433 - val_mae: 11.8433\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6761 - mae: 11.6761 - val_loss: 11.7262 - val_mae: 11.7262\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6389 - mae: 11.6389 - val_loss: 11.3366 - val_mae: 11.3366\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.5578 - mae: 11.5578 - val_loss: 11.3972 - val_mae: 11.3972\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6927 - mae: 11.6927 - val_loss: 11.8847 - val_mae: 11.8847\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5520 - mae: 11.5520 - val_loss: 11.4805 - val_mae: 11.4805\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6063 - mae: 11.6063 - val_loss: 11.3431 - val_mae: 11.3431\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6016 - mae: 11.6016 - val_loss: 11.5411 - val_mae: 11.5411\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.5973 - mae: 11.5973 - val_loss: 11.4617 - val_mae: 11.4617\n",
            "31/31 [==============================] - 0s 974us/step\n",
            "Epoch 1/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 74.5940 - mae: 74.5940 - val_loss: 24.0205 - val_mae: 24.0205\n",
            "Epoch 2/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 18.3985 - mae: 18.3985 - val_loss: 15.8093 - val_mae: 15.8093\n",
            "Epoch 3/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.3958 - mae: 16.3958 - val_loss: 16.0379 - val_mae: 16.0379\n",
            "Epoch 4/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.3272 - mae: 16.3272 - val_loss: 15.8835 - val_mae: 15.8835\n",
            "Epoch 5/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.3559 - mae: 16.3559 - val_loss: 16.0704 - val_mae: 16.0704\n",
            "Epoch 6/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2899 - mae: 16.2899 - val_loss: 15.6765 - val_mae: 15.6765\n",
            "Epoch 7/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2896 - mae: 16.2896 - val_loss: 15.8628 - val_mae: 15.8628\n",
            "Epoch 8/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2757 - mae: 16.2757 - val_loss: 16.0642 - val_mae: 16.0642\n",
            "Epoch 9/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.3335 - mae: 16.3335 - val_loss: 15.7752 - val_mae: 15.7752\n",
            "Epoch 10/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2390 - mae: 16.2390 - val_loss: 15.8592 - val_mae: 15.8592\n",
            "Epoch 11/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2926 - mae: 16.2926 - val_loss: 15.7381 - val_mae: 15.7381\n",
            "Epoch 12/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2782 - mae: 16.2782 - val_loss: 15.9445 - val_mae: 15.9445\n",
            "Epoch 13/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2290 - mae: 16.2290 - val_loss: 15.9478 - val_mae: 15.9478\n",
            "Epoch 14/100\n",
            "312/312 [==============================] - 0s 2ms/step - loss: 16.2817 - mae: 16.2817 - val_loss: 15.7151 - val_mae: 15.7151\n",
            "Epoch 15/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2972 - mae: 16.2972 - val_loss: 15.9593 - val_mae: 15.9593\n",
            "Epoch 16/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2169 - mae: 16.2169 - val_loss: 15.8387 - val_mae: 15.8387\n",
            "95/95 [==============================] - 0s 912us/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 171.4734 - mae: 171.4734 - val_loss: 157.4127 - val_mae: 157.4127\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 153.8261 - mae: 153.8261 - val_loss: 119.2699 - val_mae: 119.2699\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 73.9595 - mae: 73.9595 - val_loss: 28.1685 - val_mae: 28.1685\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 23.1763 - mae: 23.1763 - val_loss: 18.7408 - val_mae: 18.7408\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 16.5913 - mae: 16.5913 - val_loss: 14.5941 - val_mae: 14.5941\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 14.1207 - mae: 14.1207 - val_loss: 13.8367 - val_mae: 13.8367\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.2204 - mae: 13.2204 - val_loss: 12.7756 - val_mae: 12.7756\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 13.0897 - mae: 13.0897 - val_loss: 13.1911 - val_mae: 13.1911\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9962 - mae: 12.9962 - val_loss: 12.9901 - val_mae: 12.9901\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0463 - mae: 13.0463 - val_loss: 13.2676 - val_mae: 13.2676\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0644 - mae: 13.0644 - val_loss: 13.8359 - val_mae: 13.8359\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 13.0313 - mae: 13.0313 - val_loss: 13.1423 - val_mae: 13.1423\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0543 - mae: 13.0543 - val_loss: 13.0272 - val_mae: 13.0272\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.1237 - mae: 13.1237 - val_loss: 13.5168 - val_mae: 13.5168\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0011 - mae: 13.0011 - val_loss: 13.0118 - val_mae: 13.0118\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0719 - mae: 13.0719 - val_loss: 13.5275 - val_mae: 13.5275\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9334 - mae: 12.9334 - val_loss: 12.9285 - val_mae: 12.9285\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 75.8627 - mae: 75.8627 - val_loss: 24.0851 - val_mae: 24.0851\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 14.5570 - mae: 14.5570 - val_loss: 12.0466 - val_mae: 12.0466\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.9642 - mae: 11.9642 - val_loss: 11.7808 - val_mae: 11.7808\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.9134 - mae: 11.9134 - val_loss: 12.1026 - val_mae: 12.1026\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8700 - mae: 11.8700 - val_loss: 12.0387 - val_mae: 12.0387\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8827 - mae: 11.8827 - val_loss: 12.0544 - val_mae: 12.0544\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8538 - mae: 11.8538 - val_loss: 11.9879 - val_mae: 11.9879\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8342 - mae: 11.8342 - val_loss: 11.9907 - val_mae: 11.9907\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8383 - mae: 11.8383 - val_loss: 12.2626 - val_mae: 12.2626\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8557 - mae: 11.8557 - val_loss: 12.0725 - val_mae: 12.0725\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8245 - mae: 11.8245 - val_loss: 11.9966 - val_mae: 11.9966\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8292 - mae: 11.8292 - val_loss: 11.8790 - val_mae: 11.8790\n",
            "Epoch 13/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.8191 - mae: 11.8191 - val_loss: 11.8937 - val_mae: 11.8937\n",
            "97/97 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 166.3912 - mae: 166.3912 - val_loss: 161.5966 - val_mae: 161.5966\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 138.5471 - mae: 138.5471 - val_loss: 105.9800 - val_mae: 105.9800\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 59.2894 - mae: 59.2894 - val_loss: 31.8600 - val_mae: 31.8600\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 23.9312 - mae: 23.9312 - val_loss: 19.5566 - val_mae: 19.5566\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.0264 - mae: 15.0264 - val_loss: 12.4858 - val_mae: 12.4858\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.8224 - mae: 10.8224 - val_loss: 10.0250 - val_mae: 10.0250\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.4604 - mae: 10.4604 - val_loss: 10.0516 - val_mae: 10.0516\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3298 - mae: 10.3298 - val_loss: 10.2408 - val_mae: 10.2408\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3585 - mae: 10.3585 - val_loss: 10.1458 - val_mae: 10.1458\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3265 - mae: 10.3265 - val_loss: 10.4579 - val_mae: 10.4579\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.4535 - mae: 10.4535 - val_loss: 10.4441 - val_mae: 10.4441\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2046 - mae: 10.2046 - val_loss: 10.0864 - val_mae: 10.0864\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3829 - mae: 10.3829 - val_loss: 10.0400 - val_mae: 10.0400\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2028 - mae: 10.2028 - val_loss: 9.8466 - val_mae: 9.8466\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2695 - mae: 10.2695 - val_loss: 10.0580 - val_mae: 10.0580\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2520 - mae: 10.2520 - val_loss: 10.4315 - val_mae: 10.4315\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2210 - mae: 10.2210 - val_loss: 9.8924 - val_mae: 9.8924\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3152 - mae: 10.3152 - val_loss: 9.8368 - val_mae: 9.8368\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3173 - mae: 10.3173 - val_loss: 9.9927 - val_mae: 9.9927\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.3586 - mae: 10.3586 - val_loss: 9.8820 - val_mae: 9.8820\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.2361 - mae: 10.2361 - val_loss: 10.0784 - val_mae: 10.0784\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3431 - mae: 10.3431 - val_loss: 9.8929 - val_mae: 9.8929\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.2081 - mae: 10.2081 - val_loss: 9.9812 - val_mae: 9.9812\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.2314 - mae: 10.2314 - val_loss: 9.7920 - val_mae: 9.7920\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.2632 - mae: 10.2632 - val_loss: 9.8555 - val_mae: 9.8555\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1782 - mae: 10.1782 - val_loss: 9.8715 - val_mae: 9.8715\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2015 - mae: 10.2015 - val_loss: 9.8271 - val_mae: 9.8271\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2532 - mae: 10.2532 - val_loss: 10.2346 - val_mae: 10.2346\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.3020 - mae: 10.3020 - val_loss: 9.8537 - val_mae: 9.8537\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1750 - mae: 10.1750 - val_loss: 9.9434 - val_mae: 9.9434\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1982 - mae: 10.1982 - val_loss: 10.1300 - val_mae: 10.1300\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.4076 - mae: 10.4076 - val_loss: 10.0710 - val_mae: 10.0710\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2468 - mae: 10.2468 - val_loss: 10.1807 - val_mae: 10.1807\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1854 - mae: 10.1854 - val_loss: 10.1033 - val_mae: 10.1033\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 94.6707 - mae: 94.6707 - val_loss: 22.6462 - val_mae: 22.6462\n",
            "Epoch 2/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 16.3046 - mae: 16.3046 - val_loss: 14.7962 - val_mae: 14.7962\n",
            "Epoch 3/100\n",
            "309/309 [==============================] - 1s 4ms/step - loss: 15.1403 - mae: 15.1403 - val_loss: 14.7643 - val_mae: 14.7643\n",
            "Epoch 4/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.1177 - mae: 15.1177 - val_loss: 14.6921 - val_mae: 14.6921\n",
            "Epoch 5/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.1412 - mae: 15.1412 - val_loss: 14.6960 - val_mae: 14.6960\n",
            "Epoch 6/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.1004 - mae: 15.1004 - val_loss: 14.7299 - val_mae: 14.7299\n",
            "Epoch 7/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0331 - mae: 15.0331 - val_loss: 14.5845 - val_mae: 14.5845\n",
            "Epoch 8/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.1077 - mae: 15.1077 - val_loss: 14.7732 - val_mae: 14.7732\n",
            "Epoch 9/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0341 - mae: 15.0341 - val_loss: 14.6715 - val_mae: 14.6715\n",
            "Epoch 10/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0439 - mae: 15.0439 - val_loss: 14.8862 - val_mae: 14.8862\n",
            "Epoch 11/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0590 - mae: 15.0590 - val_loss: 14.6453 - val_mae: 14.6453\n",
            "Epoch 12/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0883 - mae: 15.0883 - val_loss: 14.5358 - val_mae: 14.5358\n",
            "Epoch 13/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0436 - mae: 15.0436 - val_loss: 14.7745 - val_mae: 14.7745\n",
            "Epoch 14/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9990 - mae: 14.9990 - val_loss: 14.7224 - val_mae: 14.7224\n",
            "Epoch 15/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0233 - mae: 15.0233 - val_loss: 14.6080 - val_mae: 14.6080\n",
            "Epoch 16/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0832 - mae: 15.0832 - val_loss: 14.7105 - val_mae: 14.7105\n",
            "Epoch 17/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0572 - mae: 15.0572 - val_loss: 14.9131 - val_mae: 14.9131\n",
            "Epoch 18/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0247 - mae: 15.0247 - val_loss: 15.0863 - val_mae: 15.0863\n",
            "Epoch 19/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0436 - mae: 15.0436 - val_loss: 14.7773 - val_mae: 14.7773\n",
            "Epoch 20/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0061 - mae: 15.0061 - val_loss: 14.7746 - val_mae: 14.7746\n",
            "Epoch 21/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0381 - mae: 15.0381 - val_loss: 14.7224 - val_mae: 14.7224\n",
            "Epoch 22/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.0434 - mae: 15.0434 - val_loss: 14.7768 - val_mae: 14.7768\n",
            "99/99 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - 1s 3ms/step - loss: 228.0635 - mae: 228.0635 - val_loss: 210.7646 - val_mae: 210.7646\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 212.8934 - mae: 212.8934 - val_loss: 176.0773 - val_mae: 176.0773\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 122.2886 - mae: 122.2886 - val_loss: 42.8060 - val_mae: 42.8060\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 29.3886 - mae: 29.3886 - val_loss: 21.1836 - val_mae: 21.1836\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 18.1347 - mae: 18.1347 - val_loss: 17.3895 - val_mae: 17.3895\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.2375 - mae: 16.2375 - val_loss: 16.9900 - val_mae: 16.9900\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.0800 - mae: 16.0800 - val_loss: 17.0871 - val_mae: 17.0871\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 16.0273 - mae: 16.0273 - val_loss: 17.2017 - val_mae: 17.2017\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.0710 - mae: 16.0710 - val_loss: 17.0478 - val_mae: 17.0478\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.2472 - mae: 16.2472 - val_loss: 17.0486 - val_mae: 17.0486\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1103 - mae: 16.1103 - val_loss: 17.0575 - val_mae: 17.0575\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1762 - mae: 16.1762 - val_loss: 17.0001 - val_mae: 17.0001\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 16.1092 - mae: 16.1092 - val_loss: 16.7260 - val_mae: 16.7260\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.0535 - mae: 16.0535 - val_loss: 17.2802 - val_mae: 17.2802\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1061 - mae: 16.1061 - val_loss: 17.1409 - val_mae: 17.1409\n",
            "Epoch 16/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.3323 - mae: 16.3323 - val_loss: 17.0720 - val_mae: 17.0720\n",
            "Epoch 17/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1993 - mae: 16.1993 - val_loss: 17.6210 - val_mae: 17.6210\n",
            "Epoch 18/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.0734 - mae: 16.0734 - val_loss: 17.2554 - val_mae: 17.2554\n",
            "Epoch 19/100\n",
            "95/95 [==============================] - 0s 4ms/step - loss: 16.0651 - mae: 16.0651 - val_loss: 17.1916 - val_mae: 17.1916\n",
            "Epoch 20/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1297 - mae: 16.1297 - val_loss: 16.7467 - val_mae: 16.7467\n",
            "Epoch 21/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9616 - mae: 15.9616 - val_loss: 16.9248 - val_mae: 16.9248\n",
            "Epoch 22/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.1234 - mae: 16.1234 - val_loss: 16.8856 - val_mae: 16.8856\n",
            "Epoch 23/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 16.3080 - mae: 16.3080 - val_loss: 17.0865 - val_mae: 17.0865\n",
            "29/29 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 3ms/step - loss: 96.4591 - mae: 96.4591 - val_loss: 35.8497 - val_mae: 35.8497\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 23.6199 - mae: 23.6199 - val_loss: 20.6625 - val_mae: 20.6625\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1941 - mae: 20.1941 - val_loss: 20.7815 - val_mae: 20.7815\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1847 - mae: 20.1847 - val_loss: 20.5915 - val_mae: 20.5915\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.2170 - mae: 20.2170 - val_loss: 20.6967 - val_mae: 20.6967\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1674 - mae: 20.1674 - val_loss: 20.5086 - val_mae: 20.5086\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0972 - mae: 20.0972 - val_loss: 20.6212 - val_mae: 20.6212\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1410 - mae: 20.1410 - val_loss: 20.7570 - val_mae: 20.7570\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1401 - mae: 20.1401 - val_loss: 20.5836 - val_mae: 20.5836\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1252 - mae: 20.1252 - val_loss: 20.5219 - val_mae: 20.5219\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0833 - mae: 20.0833 - val_loss: 20.8288 - val_mae: 20.8288\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1341 - mae: 20.1341 - val_loss: 20.8287 - val_mae: 20.8287\n",
            "Epoch 13/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1414 - mae: 20.1414 - val_loss: 20.7319 - val_mae: 20.7319\n",
            "Epoch 14/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0785 - mae: 20.0785 - val_loss: 20.5369 - val_mae: 20.5369\n",
            "Epoch 15/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1557 - mae: 20.1557 - val_loss: 20.6477 - val_mae: 20.6477\n",
            "Epoch 16/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.1397 - mae: 20.1397 - val_loss: 20.5753 - val_mae: 20.5753\n",
            "97/97 [==============================] - 0s 918us/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 4ms/step - loss: 226.9142 - mae: 226.9142 - val_loss: 226.5864 - val_mae: 226.5864\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 208.9600 - mae: 208.9600 - val_loss: 184.5597 - val_mae: 184.5597\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 121.3561 - mae: 121.3561 - val_loss: 53.3869 - val_mae: 53.3869\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 29.4344 - mae: 29.4344 - val_loss: 17.8771 - val_mae: 17.8771\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 16.3667 - mae: 16.3667 - val_loss: 14.4783 - val_mae: 14.4783\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4563 - mae: 15.4563 - val_loss: 14.0846 - val_mae: 14.0846\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.6488 - mae: 15.6488 - val_loss: 14.0661 - val_mae: 14.0661\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.4067 - mae: 15.4067 - val_loss: 14.1931 - val_mae: 14.1931\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.6350 - mae: 15.6350 - val_loss: 14.4287 - val_mae: 14.4287\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.4139 - mae: 15.4139 - val_loss: 14.2196 - val_mae: 14.2196\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.5408 - mae: 15.5408 - val_loss: 14.2402 - val_mae: 14.2402\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.4901 - mae: 15.4901 - val_loss: 13.9645 - val_mae: 13.9645\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.4513 - mae: 15.4513 - val_loss: 14.1256 - val_mae: 14.1256\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.3826 - mae: 15.3826 - val_loss: 14.0952 - val_mae: 14.0952\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4397 - mae: 15.4397 - val_loss: 14.2217 - val_mae: 14.2217\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5637 - mae: 15.5637 - val_loss: 14.2453 - val_mae: 14.2453\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.5999 - mae: 15.5999 - val_loss: 14.0696 - val_mae: 14.0696\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4534 - mae: 15.4534 - val_loss: 14.2537 - val_mae: 14.2537\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4252 - mae: 15.4252 - val_loss: 14.1333 - val_mae: 14.1333\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4874 - mae: 15.4874 - val_loss: 14.0521 - val_mae: 14.0521\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3669 - mae: 15.3669 - val_loss: 13.8563 - val_mae: 13.8563\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4701 - mae: 15.4701 - val_loss: 14.0953 - val_mae: 14.0953\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5307 - mae: 15.5307 - val_loss: 14.2045 - val_mae: 14.2045\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5177 - mae: 15.5177 - val_loss: 14.2547 - val_mae: 14.2547\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4301 - mae: 15.4301 - val_loss: 14.1948 - val_mae: 14.1948\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3447 - mae: 15.3447 - val_loss: 14.0115 - val_mae: 14.0115\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4123 - mae: 15.4123 - val_loss: 14.1164 - val_mae: 14.1164\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4131 - mae: 15.4131 - val_loss: 13.8956 - val_mae: 13.8956\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3559 - mae: 15.3559 - val_loss: 13.9257 - val_mae: 13.9257\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3448 - mae: 15.3448 - val_loss: 14.1011 - val_mae: 14.1011\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5139 - mae: 15.5139 - val_loss: 13.8117 - val_mae: 13.8117\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4074 - mae: 15.4074 - val_loss: 14.2700 - val_mae: 14.2700\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3260 - mae: 15.3260 - val_loss: 13.9313 - val_mae: 13.9313\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4289 - mae: 15.4289 - val_loss: 14.4785 - val_mae: 14.4785\n",
            "Epoch 35/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5106 - mae: 15.5106 - val_loss: 13.8625 - val_mae: 13.8625\n",
            "Epoch 36/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3549 - mae: 15.3549 - val_loss: 13.8913 - val_mae: 13.8913\n",
            "Epoch 37/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.5833 - mae: 15.5833 - val_loss: 14.5025 - val_mae: 14.5025\n",
            "Epoch 38/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.3227 - mae: 15.3227 - val_loss: 14.3317 - val_mae: 14.3317\n",
            "Epoch 39/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.3525 - mae: 15.3525 - val_loss: 14.4962 - val_mae: 14.4962\n",
            "Epoch 40/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.5051 - mae: 15.5051 - val_loss: 14.0249 - val_mae: 14.0249\n",
            "Epoch 41/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3441 - mae: 15.3441 - val_loss: 13.8549 - val_mae: 13.8549\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 3ms/step - loss: 144.4121 - mae: 144.4121 - val_loss: 19.4985 - val_mae: 19.4985\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 9.1102 - mae: 9.1102 - val_loss: 8.1674 - val_mae: 8.1674\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3740 - mae: 8.3740 - val_loss: 8.2534 - val_mae: 8.2534\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3468 - mae: 8.3468 - val_loss: 7.9386 - val_mae: 7.9386\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3182 - mae: 8.3182 - val_loss: 8.0879 - val_mae: 8.0879\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3258 - mae: 8.3258 - val_loss: 7.9821 - val_mae: 7.9821\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2747 - mae: 8.2747 - val_loss: 7.9257 - val_mae: 7.9257\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3086 - mae: 8.3086 - val_loss: 7.8029 - val_mae: 7.8029\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3188 - mae: 8.3188 - val_loss: 8.0284 - val_mae: 8.0284\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2819 - mae: 8.2819 - val_loss: 8.0153 - val_mae: 8.0153\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2568 - mae: 8.2568 - val_loss: 7.9626 - val_mae: 7.9626\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2743 - mae: 8.2743 - val_loss: 7.8955 - val_mae: 7.8955\n",
            "Epoch 13/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2670 - mae: 8.2670 - val_loss: 8.0648 - val_mae: 8.0648\n",
            "Epoch 14/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3265 - mae: 8.3265 - val_loss: 8.3479 - val_mae: 8.3479\n",
            "Epoch 15/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2521 - mae: 8.2521 - val_loss: 8.3481 - val_mae: 8.3481\n",
            "Epoch 16/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3358 - mae: 8.3358 - val_loss: 7.7838 - val_mae: 7.7838\n",
            "Epoch 17/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2870 - mae: 8.2870 - val_loss: 8.1171 - val_mae: 8.1171\n",
            "Epoch 18/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3336 - mae: 8.3336 - val_loss: 8.0181 - val_mae: 8.0181\n",
            "Epoch 19/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1535 - mae: 8.1535 - val_loss: 7.9177 - val_mae: 7.9177\n",
            "Epoch 20/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2162 - mae: 8.2162 - val_loss: 8.0307 - val_mae: 8.0307\n",
            "Epoch 21/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2573 - mae: 8.2573 - val_loss: 7.9808 - val_mae: 7.9808\n",
            "Epoch 22/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2076 - mae: 8.2076 - val_loss: 8.1230 - val_mae: 8.1230\n",
            "Epoch 23/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3024 - mae: 8.3024 - val_loss: 8.0078 - val_mae: 8.0078\n",
            "Epoch 24/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2176 - mae: 8.2176 - val_loss: 7.7713 - val_mae: 7.7713\n",
            "Epoch 25/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1782 - mae: 8.1782 - val_loss: 8.0260 - val_mae: 8.0260\n",
            "Epoch 26/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1915 - mae: 8.1915 - val_loss: 7.7944 - val_mae: 7.7944\n",
            "Epoch 27/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2341 - mae: 8.2341 - val_loss: 7.7850 - val_mae: 7.7850\n",
            "Epoch 28/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1902 - mae: 8.1902 - val_loss: 7.9005 - val_mae: 7.9005\n",
            "Epoch 29/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2322 - mae: 8.2322 - val_loss: 7.8039 - val_mae: 7.8039\n",
            "Epoch 30/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2298 - mae: 8.2298 - val_loss: 7.8413 - val_mae: 7.8413\n",
            "Epoch 31/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2644 - mae: 8.2644 - val_loss: 7.9351 - val_mae: 7.9351\n",
            "Epoch 32/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2393 - mae: 8.2393 - val_loss: 7.7443 - val_mae: 7.7443\n",
            "Epoch 33/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2116 - mae: 8.2116 - val_loss: 7.8513 - val_mae: 7.8513\n",
            "Epoch 34/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2207 - mae: 8.2207 - val_loss: 7.9680 - val_mae: 7.9680\n",
            "Epoch 35/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2389 - mae: 8.2389 - val_loss: 7.8473 - val_mae: 7.8473\n",
            "Epoch 36/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2173 - mae: 8.2173 - val_loss: 8.3546 - val_mae: 8.3546\n",
            "Epoch 37/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2405 - mae: 8.2405 - val_loss: 7.7422 - val_mae: 7.7422\n",
            "Epoch 38/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1970 - mae: 8.1970 - val_loss: 7.8647 - val_mae: 7.8647\n",
            "Epoch 39/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2367 - mae: 8.2367 - val_loss: 7.9269 - val_mae: 7.9269\n",
            "Epoch 40/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2318 - mae: 8.2318 - val_loss: 7.7292 - val_mae: 7.7292\n",
            "Epoch 41/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1672 - mae: 8.1672 - val_loss: 7.8999 - val_mae: 7.8999\n",
            "Epoch 42/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1541 - mae: 8.1541 - val_loss: 7.9636 - val_mae: 7.9636\n",
            "Epoch 43/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1748 - mae: 8.1748 - val_loss: 8.0519 - val_mae: 8.0519\n",
            "Epoch 44/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2236 - mae: 8.2236 - val_loss: 7.8171 - val_mae: 7.8171\n",
            "Epoch 45/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1693 - mae: 8.1693 - val_loss: 7.7499 - val_mae: 7.7499\n",
            "Epoch 46/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1987 - mae: 8.1987 - val_loss: 8.0320 - val_mae: 8.0320\n",
            "Epoch 47/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1942 - mae: 8.1942 - val_loss: 7.8814 - val_mae: 7.8814\n",
            "Epoch 48/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2081 - mae: 8.2081 - val_loss: 8.0024 - val_mae: 8.0024\n",
            "Epoch 49/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1840 - mae: 8.1840 - val_loss: 7.9755 - val_mae: 7.9755\n",
            "Epoch 50/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2732 - mae: 8.2732 - val_loss: 8.3544 - val_mae: 8.3544\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 282.1071 - mae: 282.1071 - val_loss: 286.7547 - val_mae: 286.7547\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 268.4896 - mae: 268.4896 - val_loss: 252.8915 - val_mae: 252.8915\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 183.7394 - mae: 183.7394 - val_loss: 100.8114 - val_mae: 100.8114\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 32.7320 - mae: 32.7320 - val_loss: 8.7834 - val_mae: 8.7834\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.0036 - mae: 8.0036 - val_loss: 7.5749 - val_mae: 7.5749\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.5309 - mae: 7.5309 - val_loss: 7.0545 - val_mae: 7.0545\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.6241 - mae: 7.6241 - val_loss: 6.8708 - val_mae: 6.8708\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.6263 - mae: 7.6263 - val_loss: 7.2218 - val_mae: 7.2218\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.4714 - mae: 7.4714 - val_loss: 6.9058 - val_mae: 6.9058\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.4950 - mae: 7.4950 - val_loss: 7.1938 - val_mae: 7.1938\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.5667 - mae: 7.5667 - val_loss: 7.3158 - val_mae: 7.3158\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.6164 - mae: 7.6164 - val_loss: 7.3729 - val_mae: 7.3729\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.4997 - mae: 7.4997 - val_loss: 7.2535 - val_mae: 7.2535\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.4516 - mae: 7.4516 - val_loss: 7.0754 - val_mae: 7.0754\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.3816 - mae: 7.3816 - val_loss: 7.1631 - val_mae: 7.1631\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.5307 - mae: 7.5307 - val_loss: 7.1845 - val_mae: 7.1845\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.5085 - mae: 7.5085 - val_loss: 7.3646 - val_mae: 7.3646\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 122.6439 - mae: 122.6439 - val_loss: 17.5514 - val_mae: 17.5514\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 10.1611 - mae: 10.1611 - val_loss: 9.4371 - val_mae: 9.4371\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5032 - mae: 9.5032 - val_loss: 9.3396 - val_mae: 9.3396\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.4980 - mae: 9.4980 - val_loss: 9.5487 - val_mae: 9.5487\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 9.5534 - mae: 9.5534 - val_loss: 9.3418 - val_mae: 9.3418\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5542 - mae: 9.5542 - val_loss: 9.2351 - val_mae: 9.2351\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5778 - mae: 9.5778 - val_loss: 9.4627 - val_mae: 9.4627\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5080 - mae: 9.5080 - val_loss: 9.5136 - val_mae: 9.5136\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5711 - mae: 9.5711 - val_loss: 9.7065 - val_mae: 9.7065\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5310 - mae: 9.5310 - val_loss: 9.4808 - val_mae: 9.4808\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5280 - mae: 9.5280 - val_loss: 9.4895 - val_mae: 9.4895\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5580 - mae: 9.5580 - val_loss: 9.3666 - val_mae: 9.3666\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.4271 - mae: 9.4271 - val_loss: 9.3411 - val_mae: 9.3411\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5313 - mae: 9.5313 - val_loss: 9.4432 - val_mae: 9.4432\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.5385 - mae: 9.5385 - val_loss: 9.6780 - val_mae: 9.6780\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.4407 - mae: 9.4407 - val_loss: 9.2354 - val_mae: 9.2354\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 4ms/step - loss: 253.7186 - mae: 253.7186 - val_loss: 252.7011 - val_mae: 252.7011\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 237.9510 - mae: 237.9510 - val_loss: 214.8949 - val_mae: 214.8949\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 149.4149 - mae: 149.4149 - val_loss: 64.4471 - val_mae: 64.4471\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 24.0546 - mae: 24.0546 - val_loss: 9.1707 - val_mae: 9.1707\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.5001 - mae: 8.5001 - val_loss: 9.1488 - val_mae: 9.1488\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.4001 - mae: 8.4001 - val_loss: 8.9594 - val_mae: 8.9594\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.6472 - mae: 8.6472 - val_loss: 8.6513 - val_mae: 8.6513\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2288 - mae: 8.2288 - val_loss: 8.4915 - val_mae: 8.4915\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.3266 - mae: 8.3266 - val_loss: 8.5327 - val_mae: 8.5327\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.4567 - mae: 8.4567 - val_loss: 9.3879 - val_mae: 9.3879\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.4607 - mae: 8.4607 - val_loss: 9.0413 - val_mae: 9.0413\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.4693 - mae: 8.4693 - val_loss: 9.4226 - val_mae: 9.4226\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.4947 - mae: 8.4947 - val_loss: 8.7462 - val_mae: 8.7462\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.3359 - mae: 8.3359 - val_loss: 8.7456 - val_mae: 8.7456\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.3223 - mae: 8.3223 - val_loss: 8.9405 - val_mae: 8.9405\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.3034 - mae: 8.3034 - val_loss: 8.5610 - val_mae: 8.5610\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.3255 - mae: 8.3255 - val_loss: 8.6795 - val_mae: 8.6795\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.3705 - mae: 8.3705 - val_loss: 8.7427 - val_mae: 8.7427\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 113.0067 - mae: 113.0067 - val_loss: 15.4861 - val_mae: 15.4861\n",
            "Epoch 2/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.3687 - mae: 13.3687 - val_loss: 13.0910 - val_mae: 13.0910\n",
            "Epoch 3/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1753 - mae: 13.1753 - val_loss: 13.0524 - val_mae: 13.0524\n",
            "Epoch 4/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2612 - mae: 13.2612 - val_loss: 13.2419 - val_mae: 13.2419\n",
            "Epoch 5/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1671 - mae: 13.1671 - val_loss: 13.3324 - val_mae: 13.3324\n",
            "Epoch 6/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2401 - mae: 13.2401 - val_loss: 13.4356 - val_mae: 13.4356\n",
            "Epoch 7/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1958 - mae: 13.1958 - val_loss: 13.3674 - val_mae: 13.3674\n",
            "Epoch 8/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2448 - mae: 13.2448 - val_loss: 13.0701 - val_mae: 13.0701\n",
            "Epoch 9/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2266 - mae: 13.2266 - val_loss: 13.1541 - val_mae: 13.1541\n",
            "Epoch 10/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1885 - mae: 13.1885 - val_loss: 13.3073 - val_mae: 13.3073\n",
            "Epoch 11/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2098 - mae: 13.2098 - val_loss: 13.0182 - val_mae: 13.0182\n",
            "Epoch 12/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2371 - mae: 13.2371 - val_loss: 13.3496 - val_mae: 13.3496\n",
            "Epoch 13/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1203 - mae: 13.1203 - val_loss: 13.6065 - val_mae: 13.6065\n",
            "Epoch 14/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2580 - mae: 13.2580 - val_loss: 13.0517 - val_mae: 13.0517\n",
            "Epoch 15/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1990 - mae: 13.1990 - val_loss: 12.9773 - val_mae: 12.9773\n",
            "Epoch 16/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1979 - mae: 13.1979 - val_loss: 13.2596 - val_mae: 13.2596\n",
            "Epoch 17/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2045 - mae: 13.2045 - val_loss: 13.5500 - val_mae: 13.5500\n",
            "Epoch 18/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2214 - mae: 13.2214 - val_loss: 13.0666 - val_mae: 13.0666\n",
            "Epoch 19/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1717 - mae: 13.1717 - val_loss: 13.0503 - val_mae: 13.0503\n",
            "Epoch 20/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1465 - mae: 13.1465 - val_loss: 12.9964 - val_mae: 12.9964\n",
            "Epoch 21/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1226 - mae: 13.1226 - val_loss: 13.1293 - val_mae: 13.1293\n",
            "Epoch 22/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1719 - mae: 13.1719 - val_loss: 13.0507 - val_mae: 13.0507\n",
            "Epoch 23/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.1723 - mae: 13.1723 - val_loss: 13.0235 - val_mae: 13.0235\n",
            "Epoch 24/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0990 - mae: 13.0990 - val_loss: 13.2322 - val_mae: 13.2322\n",
            "Epoch 25/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2036 - mae: 13.2036 - val_loss: 13.2678 - val_mae: 13.2678\n",
            "101/101 [==============================] - 0s 992us/step\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - 1s 3ms/step - loss: 250.6771 - mae: 250.6771 - val_loss: 239.0231 - val_mae: 239.0231\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 233.0154 - mae: 233.0154 - val_loss: 199.2773 - val_mae: 199.2773\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 141.1921 - mae: 141.1921 - val_loss: 56.8914 - val_mae: 56.8914\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 26.3534 - mae: 26.3534 - val_loss: 12.2891 - val_mae: 12.2891\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.8387 - mae: 11.8387 - val_loss: 10.4611 - val_mae: 10.4611\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.5796 - mae: 11.5796 - val_loss: 10.5129 - val_mae: 10.5129\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.6195 - mae: 11.6195 - val_loss: 10.8965 - val_mae: 10.8965\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.8889 - mae: 11.8889 - val_loss: 10.7302 - val_mae: 10.7302\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.5493 - mae: 11.5493 - val_loss: 11.1198 - val_mae: 11.1198\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.4290 - mae: 11.4290 - val_loss: 10.9820 - val_mae: 10.9820\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.5829 - mae: 11.5829 - val_loss: 10.6611 - val_mae: 10.6611\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.7194 - mae: 11.7194 - val_loss: 10.6653 - val_mae: 10.6653\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.5251 - mae: 11.5251 - val_loss: 10.7090 - val_mae: 10.7090\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.5891 - mae: 11.5891 - val_loss: 10.8810 - val_mae: 10.8810\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.5029 - mae: 11.5029 - val_loss: 10.9859 - val_mae: 10.9859\n",
            "29/29 [==============================] - 0s 2ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 115.0125 - mae: 115.0125 - val_loss: 23.7195 - val_mae: 23.7195\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 7.9450 - mae: 7.9450 - val_loss: 5.7319 - val_mae: 5.7319\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.8402 - mae: 5.8402 - val_loss: 5.8918 - val_mae: 5.8918\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7870 - mae: 5.7870 - val_loss: 5.9422 - val_mae: 5.9422\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7024 - mae: 5.7024 - val_loss: 5.8858 - val_mae: 5.8858\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7873 - mae: 5.7873 - val_loss: 5.5993 - val_mae: 5.5993\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7956 - mae: 5.7956 - val_loss: 5.9354 - val_mae: 5.9354\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7620 - mae: 5.7620 - val_loss: 5.5578 - val_mae: 5.5578\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7128 - mae: 5.7128 - val_loss: 5.6626 - val_mae: 5.6626\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7228 - mae: 5.7228 - val_loss: 5.9256 - val_mae: 5.9256\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7363 - mae: 5.7363 - val_loss: 5.9824 - val_mae: 5.9824\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.8119 - mae: 5.8119 - val_loss: 5.9738 - val_mae: 5.9738\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7731 - mae: 5.7731 - val_loss: 5.5248 - val_mae: 5.5248\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7491 - mae: 5.7491 - val_loss: 5.5505 - val_mae: 5.5505\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7319 - mae: 5.7319 - val_loss: 5.4746 - val_mae: 5.4746\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.8147 - mae: 5.8147 - val_loss: 5.4830 - val_mae: 5.4830\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7570 - mae: 5.7570 - val_loss: 5.7042 - val_mae: 5.7042\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.6344 - mae: 5.6344 - val_loss: 5.5195 - val_mae: 5.5195\n",
            "Epoch 19/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7773 - mae: 5.7773 - val_loss: 5.5423 - val_mae: 5.5423\n",
            "Epoch 20/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.6842 - mae: 5.6842 - val_loss: 5.5816 - val_mae: 5.5816\n",
            "Epoch 21/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7670 - mae: 5.7670 - val_loss: 5.5287 - val_mae: 5.5287\n",
            "Epoch 22/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.6920 - mae: 5.6920 - val_loss: 5.6297 - val_mae: 5.6297\n",
            "Epoch 23/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.6859 - mae: 5.6859 - val_loss: 5.8326 - val_mae: 5.8326\n",
            "Epoch 24/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7222 - mae: 5.7222 - val_loss: 5.5734 - val_mae: 5.5734\n",
            "Epoch 25/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.7499 - mae: 5.7499 - val_loss: 5.6173 - val_mae: 5.6173\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 242.1135 - mae: 242.1135 - val_loss: 236.6029 - val_mae: 236.6029\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 223.9699 - mae: 223.9699 - val_loss: 196.0987 - val_mae: 196.0987\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 144.4814 - mae: 144.4814 - val_loss: 69.8872 - val_mae: 69.8872\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 38.5580 - mae: 38.5580 - val_loss: 16.5087 - val_mae: 16.5087\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.3346 - mae: 8.3346 - val_loss: 5.2679 - val_mae: 5.2679\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.6897 - mae: 5.6897 - val_loss: 5.1907 - val_mae: 5.1907\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.5109 - mae: 5.5109 - val_loss: 5.3828 - val_mae: 5.3828\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.4133 - mae: 5.4133 - val_loss: 5.3801 - val_mae: 5.3801\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.4036 - mae: 5.4036 - val_loss: 4.8196 - val_mae: 4.8196\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.3933 - mae: 5.3933 - val_loss: 5.1851 - val_mae: 5.1851\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.2639 - mae: 5.2639 - val_loss: 5.3291 - val_mae: 5.3291\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.3239 - mae: 5.3239 - val_loss: 5.2093 - val_mae: 5.2093\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.7599 - mae: 5.7599 - val_loss: 5.4842 - val_mae: 5.4842\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.4292 - mae: 5.4292 - val_loss: 4.9702 - val_mae: 4.9702\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.4531 - mae: 5.4531 - val_loss: 5.6755 - val_mae: 5.6755\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.5027 - mae: 5.5027 - val_loss: 5.1780 - val_mae: 5.1780\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.3524 - mae: 5.3524 - val_loss: 5.3295 - val_mae: 5.3295\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.4859 - mae: 5.4859 - val_loss: 5.2021 - val_mae: 5.2021\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.5104 - mae: 5.5104 - val_loss: 4.9467 - val_mae: 4.9467\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Overall R^2 for Neural Network: 0.9879339571686392\n",
            "Overall MAE for Neural Network: 0\n",
            "    stop_sequence day_type       R^2        MAE\n",
            "0               1  weekday  0.895528  27.367165\n",
            "1               1  weekend  0.961258  26.270143\n",
            "2               5  weekday  0.979687  14.288163\n",
            "3               5  weekend  0.991578  11.756729\n",
            "4               8  weekday  0.978668  16.035363\n",
            "5               8  weekend  0.993043  12.487151\n",
            "6              12  weekday  0.991060  11.483520\n",
            "7              12  weekend  0.995147  10.268472\n",
            "8              14  weekday  0.986573  14.904634\n",
            "9              14  weekend  0.991117  15.714723\n",
            "10             16  weekday  0.974748  19.460224\n",
            "11             16  weekend  0.992383  14.679975\n",
            "12             19  weekday  0.995501   7.896635\n",
            "13             19  weekend  0.997684   7.188865\n",
            "14             20  weekday  0.994524   9.275202\n",
            "15             20  weekend  0.997911   7.457516\n",
            "16             21  weekday  0.991342  13.220764\n",
            "17             21  weekend  0.996285  10.602676\n",
            "18             23  weekday  0.998149   5.555361\n",
            "19             23  weekend  0.999032   4.776689\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "def train_and_evaluate_nn(df):\n",
        "    # Split the dataset into training and test sets\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)  # Use df, not df_selected_stops\n",
        "\n",
        "    if df_train.empty or df_test.empty:\n",
        "        print(\"Training or testing dataset is empty.\")\n",
        "        return None  # Return early if the dataset is empty\n",
        "\n",
        "    sc = StandardScaler()\n",
        "\n",
        "    y_mean = df_test['arrival_delay'].mean()\n",
        "    overall_ssr = 0  # Initialize overall_ssr\n",
        "    overall_tss = 0  # Initialize overall_tss\n",
        "    overall_mae = 0  # Initialize overall MAE\n",
        "\n",
        "    results = {\n",
        "        'stop_sequence': [],\n",
        "        'day_type': [],\n",
        "        'R^2': [],\n",
        "        'MAE': []\n",
        "    }\n",
        "\n",
        "    for stop_seq in df['stop_sequence'].unique():\n",
        "        for day_type in ['weekday', 'weekend']:\n",
        "            # Filter data\n",
        "            df_train_subset = df_train[(df_train['stop_sequence'] == stop_seq) & (df_train['day_of_week'] == day_type)]\n",
        "            df_test_subset = df_test[(df_test['stop_sequence'] == stop_seq) & (df_test['day_of_week'] == day_type)]\n",
        "\n",
        "            # Prepare data\n",
        "            x_train = df_train_subset.drop(['arrival_delay', 'day_of_week', 'time_of_day', \"weather\", \"temperature\"], axis=1)\n",
        "            y_train = df_train_subset['arrival_delay']\n",
        "\n",
        "            x_test = df_test_subset.drop(['arrival_delay', 'day_of_week', 'time_of_day', \"weather\", \"temperature\"], axis=1)\n",
        "            y_test = df_test_subset['arrival_delay']\n",
        "\n",
        "            # Normalize the input features\n",
        "            x_train = sc.fit_transform(x_train)\n",
        "            x_test = sc.transform(x_test)\n",
        "\n",
        "            # Neural network model\n",
        "            model = Sequential()\n",
        "            model.add(Dense(32, activation='linear', input_dim=x_train.shape[1]))\n",
        "            model.add(Dropout(0.001))\n",
        "            model.add(Dense(64, activation='linear'))\n",
        "            model.add(Dense(1))\n",
        "\n",
        "            model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "            model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "            y_pred = model.predict(x_test).flatten()\n",
        "            current_r2 = r2_score(y_test, y_pred)\n",
        "            current_mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "            residuals = y_test - y_pred\n",
        "            ssr = sum(residuals**2)\n",
        "            overall_ssr += ssr\n",
        "            tss = sum((y_test - y_mean)**2)\n",
        "            overall_tss += tss\n",
        "\n",
        "            results['stop_sequence'].append(stop_seq)\n",
        "            results['day_type'].append(day_type)\n",
        "            results['R^2'].append(current_r2)\n",
        "            results['MAE'].append(current_mae)\n",
        "\n",
        "    overall_r2 = 1 - (overall_ssr / overall_tss)\n",
        "\n",
        "    print(f'Overall R^2 for Neural Network: {overall_r2}')\n",
        "    print(f'Overall MAE for Neural Network: {overall_mae}')\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "results_df = train_and_evaluate_nn(df_selected_stops)\n",
        "if results_df is not None:\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3mZt0TcTUMTi",
      "metadata": {
        "id": "3mZt0TcTUMTi"
      },
      "source": [
        "### Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "acPIeRD8UL_-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acPIeRD8UL_-",
        "outputId": "932192b7-d377-4baf-aa61-d2e8c137d785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall R^2: 0.9887827662407284\n",
            "Overall MAE: 13.757458257476697\n",
            "    stop_sequence day_type       R^2        MAE\n",
            "0               1  weekday  0.906024  28.579119\n",
            "1               1  weekend  0.962977  27.315545\n",
            "2               5  weekday  0.981150  14.408368\n",
            "3               5  weekend  0.991836  11.804386\n",
            "4               8  weekday  0.979774  16.290035\n",
            "5               8  weekend  0.993258  12.420537\n",
            "6              12  weekday  0.991950  12.045683\n",
            "7              12  weekend  0.995815  10.752941\n",
            "8              14  weekday  0.987545  14.890875\n",
            "9              14  weekend  0.991588  15.946099\n",
            "10             16  weekday  0.975844  20.081454\n",
            "11             16  weekend  0.992541  14.902667\n",
            "12             19  weekday  0.995727   8.051218\n",
            "13             19  weekend  0.997954   7.066374\n",
            "14             20  weekday  0.994995   9.300995\n",
            "15             20  weekend  0.998012   7.482150\n",
            "16             21  weekday  0.991727  13.204515\n",
            "17             21  weekend  0.996472  10.654456\n",
            "18             23  weekday  0.998250   5.474750\n",
            "19             23  weekend  0.999060   4.769228\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "def train_and_evaluate(df):\n",
        "    # Split the entire dataset into training and test sets\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    y_mean = df_test['arrival_delay'].mean()\n",
        "\n",
        "    results = {\n",
        "        'stop_sequence': [],\n",
        "        'day_type': [],\n",
        "        'R^2': [],\n",
        "        'MAE': []\n",
        "    }\n",
        "\n",
        "    weighted_mae_sum = 0\n",
        "    total_samples = 0\n",
        "    overall_ssr = 0\n",
        "    overall_tss = 0\n",
        "\n",
        "    for stop_seq in df['stop_sequence'].unique():\n",
        "        for day_type in ['weekday', 'weekend']:\n",
        "            # Filter data by stop sequence and day type\n",
        "            df_train_subset = df_train[(df_train['stop_sequence'] == stop_seq) & (df_train['day_of_week'] == day_type)]\n",
        "            df_test_subset = df_test[(df_test['stop_sequence'] == stop_seq) & (df_test['day_of_week'] == day_type)]\n",
        "\n",
        "            # Train model with RFECV\n",
        "            x_train = df_train_subset.drop(['arrival_delay', 'day_of_week', 'time_of_day', \"weather\", \"temperature\"], axis=1)\n",
        "            y_train = df_train_subset['arrival_delay']\n",
        "\n",
        "            model = LinearRegression()\n",
        "            selector = RFECV(estimator=model, step=1, cv=KFold(5))\n",
        "            selector = selector.fit(x_train, y_train)\n",
        "\n",
        "            # Fit model with selected features\n",
        "            model.fit(x_train.iloc[:, selector.support_], y_train)\n",
        "\n",
        "            # Evaluate the model\n",
        "            x_test = df_test_subset.drop(['arrival_delay', 'day_of_week', 'time_of_day', \"weather\", \"temperature\"], axis=1).iloc[:, selector.support_]\n",
        "            y_test = df_test_subset['arrival_delay']\n",
        "\n",
        "            y_pred = model.predict(x_test)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            sample_count = len(y_test)\n",
        "            weighted_mae_sum += mae * sample_count\n",
        "            total_samples += sample_count\n",
        "\n",
        "            residuals = y_test - y_pred\n",
        "            ssr = sum(residuals**2)\n",
        "            overall_ssr += ssr\n",
        "            tss = sum((y_test - y_mean)**2)\n",
        "            overall_tss += tss\n",
        "\n",
        "            results['stop_sequence'].append(stop_seq)\n",
        "            results['day_type'].append(day_type)\n",
        "            results['R^2'].append(r2)\n",
        "            results['MAE'].append(mae)\n",
        "\n",
        "\n",
        "    overall_r2 = 1 - (overall_ssr / overall_tss)\n",
        "    overall_mae = weighted_mae_sum / total_samples\n",
        "\n",
        "    print(f'Overall R^2: {overall_r2}')\n",
        "    print(f'Overall MAE: {overall_mae}')\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "results_df = train_and_evaluate(df_selected_stops)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LBs3sZh86PPC",
      "metadata": {
        "id": "LBs3sZh86PPC"
      },
      "source": [
        "# Model without weather"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_stop_sequences = [0, 1, 5, 8, 12, 14, 16, 19, 20, 21, 23]  # Replace with your specific stop sequence number\n",
        "\n",
        "# Create a boolean mask for rows with the desired stop sequence\n",
        "mask = df_no_outliers['stop_sequence'].isin(desired_stop_sequences)\n",
        "\n",
        "# Filter the DataFrame to keep only the rows with the desired stop sequence\n",
        "df_selected_stops2 = df_no_outliers[mask]\n",
        "\n",
        "df_selected_stops2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwi7CFlERr6",
        "outputId": "21eef7b2-4ac5-4466-87a5-93e813a678d1"
      },
      "id": "bQwi7CFlERr6",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(201801, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df_selected_stops2.drop(['Calendar_date','route_id', 'bus_id'], axis = 1)\n",
        "columns_to_drop = [\n",
        "    'weather', 'temperature', 'time_of_day',\n",
        "    'factor(weather)Light_Rain', 'factor(weather)Light_Snow',\n",
        "    'factor(weather)Normal', 'factor(weather)Rain', 'factor(weather)Snow',\n",
        "    'factor(temperature)Cold', 'factor(temperature)Extra_cold',\n",
        "    'factor(temperature)Normal', 'factor(day_of_week)weekday',\n",
        "    'factor(day_of_week)weekend', 'factor(time_of_day)Afternoon_peak',\n",
        "    'factor(time_of_day)Morning_peak', 'factor(time_of_day)Off-peak'\n",
        "]\n",
        "for column in columns_to_drop:\n",
        "    df2.pop(column)"
      ],
      "metadata": {
        "id": "mKc-1ALXGZ40"
      },
      "id": "mKc-1ALXGZ40",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nzmmA_rGel2",
        "outputId": "86a4ff49-968f-4ce9-8f2c-ba9ce2e7a846"
      },
      "id": "-nzmmA_rGel2",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 201801 entries, 0 to 545098\n",
            "Data columns (total 12 columns):\n",
            " #   Column                            Non-Null Count   Dtype \n",
            "---  ------                            --------------   ----- \n",
            " 0   stop_sequence                     201801 non-null  int64 \n",
            " 1   arrival_delay                     201801 non-null  int64 \n",
            " 2   dwell_time                        201801 non-null  int64 \n",
            " 3   travel_time_for_previous_section  201801 non-null  int64 \n",
            " 4   scheduled_travel_time             201801 non-null  int64 \n",
            " 5   upstream_stop_delay               201801 non-null  int64 \n",
            " 6   origin_delay                      201801 non-null  int64 \n",
            " 7   previous_bus_delay                201801 non-null  int64 \n",
            " 8   previous_trip_travel_time         201801 non-null  int64 \n",
            " 9   traffic_condition                 201801 non-null  int64 \n",
            " 10  recurrent_delay                   201801 non-null  int64 \n",
            " 11  day_of_week                       201801 non-null  object\n",
            "dtypes: int64(11), object(1)\n",
            "memory usage: 20.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd697ca",
      "metadata": {
        "id": "4bd697ca"
      },
      "source": [
        "### Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "898CZJgs6RYT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "898CZJgs6RYT",
        "outputId": "69e71aa3-5513-44f7-c239-2fbafd6c99a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 49.6896 - mae: 49.6896 - val_loss: 27.4155 - val_mae: 27.4155\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5674 - mae: 27.5674 - val_loss: 27.3973 - val_mae: 27.3973\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5536 - mae: 27.5536 - val_loss: 27.1925 - val_mae: 27.1925\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5427 - mae: 27.5427 - val_loss: 27.7532 - val_mae: 27.7532\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.4864 - mae: 27.4864 - val_loss: 27.2201 - val_mae: 27.2201\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5722 - mae: 27.5722 - val_loss: 27.2691 - val_mae: 27.2691\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5423 - mae: 27.5423 - val_loss: 27.2916 - val_mae: 27.2916\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5028 - mae: 27.5028 - val_loss: 27.1281 - val_mae: 27.1281\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5411 - mae: 27.5411 - val_loss: 27.1960 - val_mae: 27.1960\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5582 - mae: 27.5582 - val_loss: 27.2780 - val_mae: 27.2780\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5566 - mae: 27.5566 - val_loss: 27.2636 - val_mae: 27.2636\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5230 - mae: 27.5230 - val_loss: 27.3235 - val_mae: 27.3235\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 27.5466 - mae: 27.5466 - val_loss: 27.2468 - val_mae: 27.2468\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5185 - mae: 27.5185 - val_loss: 27.2644 - val_mae: 27.2644\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5290 - mae: 27.5290 - val_loss: 27.2128 - val_mae: 27.2128\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5613 - mae: 27.5613 - val_loss: 27.2631 - val_mae: 27.2631\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5072 - mae: 27.5072 - val_loss: 27.2690 - val_mae: 27.2690\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 27.5299 - mae: 27.5299 - val_loss: 27.3427 - val_mae: 27.3427\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 117.8999 - mae: 117.8999 - val_loss: 111.7090 - val_mae: 111.7090\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 93.5573 - mae: 93.5573 - val_loss: 66.8843 - val_mae: 66.8843\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 36.3786 - mae: 36.3786 - val_loss: 26.5188 - val_mae: 26.5188\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.4592 - mae: 26.4592 - val_loss: 26.5183 - val_mae: 26.5183\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.3585 - mae: 26.3585 - val_loss: 26.2693 - val_mae: 26.2693\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.2496 - mae: 26.2496 - val_loss: 26.0370 - val_mae: 26.0370\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1338 - mae: 26.1338 - val_loss: 25.8817 - val_mae: 25.8817\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0695 - mae: 26.0695 - val_loss: 25.7601 - val_mae: 25.7601\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1425 - mae: 26.1425 - val_loss: 25.9596 - val_mae: 25.9596\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1577 - mae: 26.1577 - val_loss: 26.0350 - val_mae: 26.0350\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1161 - mae: 26.1161 - val_loss: 25.7679 - val_mae: 25.7679\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.1687 - mae: 26.1687 - val_loss: 25.8950 - val_mae: 25.8950\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0977 - mae: 26.0977 - val_loss: 25.9296 - val_mae: 25.9296\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1152 - mae: 26.1152 - val_loss: 25.7404 - val_mae: 25.7404\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.1522 - mae: 26.1522 - val_loss: 25.9674 - val_mae: 25.9674\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1257 - mae: 26.1257 - val_loss: 25.9159 - val_mae: 25.9159\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0898 - mae: 26.0898 - val_loss: 25.9669 - val_mae: 25.9669\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0797 - mae: 26.0797 - val_loss: 25.8634 - val_mae: 25.8634\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0652 - mae: 26.0652 - val_loss: 26.1839 - val_mae: 26.1839\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.1463 - mae: 26.1463 - val_loss: 25.8363 - val_mae: 25.8363\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0508 - mae: 26.0508 - val_loss: 25.8262 - val_mae: 25.8262\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0279 - mae: 26.0279 - val_loss: 25.9290 - val_mae: 25.9290\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 26.0685 - mae: 26.0685 - val_loss: 26.0309 - val_mae: 26.0309\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 26.0514 - mae: 26.0514 - val_loss: 25.7825 - val_mae: 25.7825\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 65.9487 - mae: 65.9487 - val_loss: 19.8296 - val_mae: 19.8296\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 15.5801 - mae: 15.5801 - val_loss: 14.8135 - val_mae: 14.8135\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3528 - mae: 14.3528 - val_loss: 14.7018 - val_mae: 14.7018\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3475 - mae: 14.3475 - val_loss: 15.1517 - val_mae: 15.1517\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3408 - mae: 14.3408 - val_loss: 14.6631 - val_mae: 14.6631\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3557 - mae: 14.3557 - val_loss: 14.6925 - val_mae: 14.6925\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3375 - mae: 14.3375 - val_loss: 14.6521 - val_mae: 14.6521\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3062 - mae: 14.3062 - val_loss: 14.7827 - val_mae: 14.7827\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3158 - mae: 14.3158 - val_loss: 14.6320 - val_mae: 14.6320\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 14.3598 - mae: 14.3598 - val_loss: 14.6245 - val_mae: 14.6245\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 14.3634 - mae: 14.3634 - val_loss: 14.6230 - val_mae: 14.6230\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3762 - mae: 14.3762 - val_loss: 14.6798 - val_mae: 14.6798\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.2953 - mae: 14.2953 - val_loss: 14.7535 - val_mae: 14.7535\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3198 - mae: 14.3198 - val_loss: 14.7548 - val_mae: 14.7548\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3891 - mae: 14.3891 - val_loss: 14.5909 - val_mae: 14.5909\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3225 - mae: 14.3225 - val_loss: 14.6818 - val_mae: 14.6818\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3567 - mae: 14.3567 - val_loss: 14.6466 - val_mae: 14.6466\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3325 - mae: 14.3325 - val_loss: 14.5474 - val_mae: 14.5474\n",
            "Epoch 19/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.2865 - mae: 14.2865 - val_loss: 14.6171 - val_mae: 14.6171\n",
            "Epoch 20/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3434 - mae: 14.3434 - val_loss: 14.7030 - val_mae: 14.7030\n",
            "Epoch 21/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3339 - mae: 14.3339 - val_loss: 14.5943 - val_mae: 14.5943\n",
            "Epoch 22/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3364 - mae: 14.3364 - val_loss: 14.7489 - val_mae: 14.7489\n",
            "Epoch 23/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3227 - mae: 14.3227 - val_loss: 14.6705 - val_mae: 14.6705\n",
            "Epoch 24/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3225 - mae: 14.3225 - val_loss: 14.6063 - val_mae: 14.6063\n",
            "Epoch 25/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3180 - mae: 14.3180 - val_loss: 14.6490 - val_mae: 14.6490\n",
            "Epoch 26/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3587 - mae: 14.3587 - val_loss: 14.5499 - val_mae: 14.5499\n",
            "Epoch 27/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.2931 - mae: 14.2931 - val_loss: 14.8998 - val_mae: 14.8998\n",
            "Epoch 28/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 14.3647 - mae: 14.3647 - val_loss: 14.7106 - val_mae: 14.7106\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 155.4915 - mae: 155.4915 - val_loss: 151.0709 - val_mae: 151.0709\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 138.7354 - mae: 138.7354 - val_loss: 111.1313 - val_mae: 111.1313\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 55.0605 - mae: 55.0605 - val_loss: 18.6767 - val_mae: 18.6767\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 16.5895 - mae: 16.5895 - val_loss: 14.5199 - val_mae: 14.5199\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 13.4970 - mae: 13.4970 - val_loss: 12.5802 - val_mae: 12.5802\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 12.0287 - mae: 12.0287 - val_loss: 12.0142 - val_mae: 12.0142\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.7294 - mae: 11.7294 - val_loss: 11.5547 - val_mae: 11.5547\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5748 - mae: 11.5748 - val_loss: 11.5317 - val_mae: 11.5317\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.7104 - mae: 11.7104 - val_loss: 11.5217 - val_mae: 11.5217\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5949 - mae: 11.5949 - val_loss: 11.4459 - val_mae: 11.4459\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5966 - mae: 11.5966 - val_loss: 11.6348 - val_mae: 11.6348\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6777 - mae: 11.6777 - val_loss: 11.5733 - val_mae: 11.5733\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 11.6548 - mae: 11.6548 - val_loss: 11.4765 - val_mae: 11.4765\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6490 - mae: 11.6490 - val_loss: 11.5389 - val_mae: 11.5389\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5381 - mae: 11.5381 - val_loss: 11.6201 - val_mae: 11.6201\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.6358 - mae: 11.6358 - val_loss: 11.5103 - val_mae: 11.5103\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5723 - mae: 11.5723 - val_loss: 11.5554 - val_mae: 11.5554\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5695 - mae: 11.5695 - val_loss: 11.5279 - val_mae: 11.5279\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5747 - mae: 11.5747 - val_loss: 11.7405 - val_mae: 11.7405\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 0s 2ms/step - loss: 11.5357 - mae: 11.5357 - val_loss: 11.8398 - val_mae: 11.8398\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 70.1198 - mae: 70.1198 - val_loss: 22.5259 - val_mae: 22.5259\n",
            "Epoch 2/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 17.5916 - mae: 17.5916 - val_loss: 15.7599 - val_mae: 15.7599\n",
            "Epoch 3/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1928 - mae: 16.1928 - val_loss: 15.6394 - val_mae: 15.6394\n",
            "Epoch 4/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1467 - mae: 16.1467 - val_loss: 15.8787 - val_mae: 15.8787\n",
            "Epoch 5/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2074 - mae: 16.2074 - val_loss: 15.6474 - val_mae: 15.6474\n",
            "Epoch 6/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2129 - mae: 16.2129 - val_loss: 15.7756 - val_mae: 15.7756\n",
            "Epoch 7/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2062 - mae: 16.2062 - val_loss: 15.6636 - val_mae: 15.6636\n",
            "Epoch 8/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1871 - mae: 16.1871 - val_loss: 15.7676 - val_mae: 15.7676\n",
            "Epoch 9/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1655 - mae: 16.1655 - val_loss: 15.6615 - val_mae: 15.6615\n",
            "Epoch 10/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1789 - mae: 16.1789 - val_loss: 15.9911 - val_mae: 15.9911\n",
            "Epoch 11/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1404 - mae: 16.1404 - val_loss: 15.7868 - val_mae: 15.7868\n",
            "Epoch 12/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.2237 - mae: 16.2237 - val_loss: 15.7116 - val_mae: 15.7116\n",
            "Epoch 13/100\n",
            "312/312 [==============================] - 1s 2ms/step - loss: 16.1090 - mae: 16.1090 - val_loss: 16.0774 - val_mae: 16.0774\n",
            "95/95 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 170.4634 - mae: 170.4634 - val_loss: 154.7034 - val_mae: 154.7034\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 144.2966 - mae: 144.2966 - val_loss: 105.7239 - val_mae: 105.7239\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 58.7728 - mae: 58.7728 - val_loss: 22.8266 - val_mae: 22.8266\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 19.6569 - mae: 19.6569 - val_loss: 15.9359 - val_mae: 15.9359\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 14.6654 - mae: 14.6654 - val_loss: 13.6411 - val_mae: 13.6411\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.1509 - mae: 13.1509 - val_loss: 12.7850 - val_mae: 12.7850\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9011 - mae: 12.9011 - val_loss: 13.0015 - val_mae: 13.0015\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9225 - mae: 12.9225 - val_loss: 12.7098 - val_mae: 12.7098\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8498 - mae: 12.8498 - val_loss: 13.0328 - val_mae: 13.0328\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 12.8850 - mae: 12.8850 - val_loss: 12.7980 - val_mae: 12.7980\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8011 - mae: 12.8011 - val_loss: 12.6591 - val_mae: 12.6591\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8628 - mae: 12.8628 - val_loss: 12.8063 - val_mae: 12.8063\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8383 - mae: 12.8383 - val_loss: 12.9748 - val_mae: 12.9748\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9496 - mae: 12.9496 - val_loss: 12.9107 - val_mae: 12.9107\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.9385 - mae: 12.9385 - val_loss: 12.7744 - val_mae: 12.7744\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8878 - mae: 12.8878 - val_loss: 12.9565 - val_mae: 12.9565\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 13.0255 - mae: 13.0255 - val_loss: 12.8983 - val_mae: 12.8983\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8529 - mae: 12.8529 - val_loss: 12.7959 - val_mae: 12.7959\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8702 - mae: 12.8702 - val_loss: 12.9229 - val_mae: 12.9229\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8511 - mae: 12.8511 - val_loss: 12.7543 - val_mae: 12.7543\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.8875 - mae: 12.8875 - val_loss: 13.2437 - val_mae: 13.2437\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 70.8590 - mae: 70.8590 - val_loss: 21.0854 - val_mae: 21.0854\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 13.3290 - mae: 13.3290 - val_loss: 11.9524 - val_mae: 11.9524\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7219 - mae: 11.7219 - val_loss: 11.8426 - val_mae: 11.8426\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7371 - mae: 11.7371 - val_loss: 11.7360 - val_mae: 11.7360\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 3ms/step - loss: 11.7073 - mae: 11.7073 - val_loss: 11.7850 - val_mae: 11.7850\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7218 - mae: 11.7218 - val_loss: 11.8338 - val_mae: 11.8338\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7691 - mae: 11.7691 - val_loss: 11.8659 - val_mae: 11.8659\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7248 - mae: 11.7248 - val_loss: 11.7955 - val_mae: 11.7955\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7606 - mae: 11.7606 - val_loss: 11.7663 - val_mae: 11.7663\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7435 - mae: 11.7435 - val_loss: 11.7730 - val_mae: 11.7730\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7299 - mae: 11.7299 - val_loss: 11.8459 - val_mae: 11.8459\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7406 - mae: 11.7406 - val_loss: 11.8368 - val_mae: 11.8368\n",
            "Epoch 13/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7335 - mae: 11.7335 - val_loss: 11.8242 - val_mae: 11.8242\n",
            "Epoch 14/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 11.7481 - mae: 11.7481 - val_loss: 11.8044 - val_mae: 11.8044\n",
            "97/97 [==============================] - 0s 974us/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 4ms/step - loss: 165.7305 - mae: 165.7305 - val_loss: 159.8648 - val_mae: 159.8648\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 132.6144 - mae: 132.6144 - val_loss: 94.5690 - val_mae: 94.5690\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 50.9408 - mae: 50.9408 - val_loss: 27.4640 - val_mae: 27.4640\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 20.6140 - mae: 20.6140 - val_loss: 15.9442 - val_mae: 15.9442\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 12.4964 - mae: 12.4964 - val_loss: 10.6832 - val_mae: 10.6832\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1812 - mae: 10.1812 - val_loss: 10.1026 - val_mae: 10.1026\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1411 - mae: 10.1411 - val_loss: 9.9233 - val_mae: 9.9233\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.0995 - mae: 10.0995 - val_loss: 9.9151 - val_mae: 9.9151\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1872 - mae: 10.1872 - val_loss: 9.9303 - val_mae: 9.9303\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.0517 - mae: 10.0517 - val_loss: 10.0607 - val_mae: 10.0607\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1064 - mae: 10.1064 - val_loss: 9.8751 - val_mae: 9.8751\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1901 - mae: 10.1901 - val_loss: 9.8888 - val_mae: 9.8888\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1017 - mae: 10.1017 - val_loss: 9.9421 - val_mae: 9.9421\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1872 - mae: 10.1872 - val_loss: 9.8914 - val_mae: 9.8914\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1674 - mae: 10.1674 - val_loss: 9.8892 - val_mae: 9.8892\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.2004 - mae: 10.2004 - val_loss: 9.8993 - val_mae: 9.8993\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.0862 - mae: 10.0862 - val_loss: 10.0364 - val_mae: 10.0364\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.0595 - mae: 10.0595 - val_loss: 9.8374 - val_mae: 9.8374\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1472 - mae: 10.1472 - val_loss: 9.9046 - val_mae: 9.9046\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 10.1073 - mae: 10.1073 - val_loss: 9.9851 - val_mae: 9.9851\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.0714 - mae: 10.0714 - val_loss: 9.8921 - val_mae: 9.8921\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2441 - mae: 10.2441 - val_loss: 9.8976 - val_mae: 9.8976\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1124 - mae: 10.1124 - val_loss: 10.0326 - val_mae: 10.0326\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.1433 - mae: 10.1433 - val_loss: 9.9980 - val_mae: 9.9980\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.0734 - mae: 10.0734 - val_loss: 9.9660 - val_mae: 9.9660\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.0597 - mae: 10.0597 - val_loss: 10.0340 - val_mae: 10.0340\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.0325 - mae: 10.0325 - val_loss: 9.9563 - val_mae: 9.9563\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.2085 - mae: 10.2085 - val_loss: 9.8716 - val_mae: 9.8716\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 91.2918 - mae: 91.2918 - val_loss: 20.9209 - val_mae: 20.9209\n",
            "Epoch 2/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 15.5887 - mae: 15.5887 - val_loss: 14.6597 - val_mae: 14.6597\n",
            "Epoch 3/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9325 - mae: 14.9325 - val_loss: 14.5072 - val_mae: 14.5072\n",
            "Epoch 4/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.8971 - mae: 14.8971 - val_loss: 14.7165 - val_mae: 14.7165\n",
            "Epoch 5/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9585 - mae: 14.9585 - val_loss: 14.5858 - val_mae: 14.5858\n",
            "Epoch 6/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9011 - mae: 14.9011 - val_loss: 14.6000 - val_mae: 14.6000\n",
            "Epoch 7/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9265 - mae: 14.9265 - val_loss: 14.5615 - val_mae: 14.5615\n",
            "Epoch 8/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9437 - mae: 14.9437 - val_loss: 14.5597 - val_mae: 14.5597\n",
            "Epoch 9/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.8996 - mae: 14.8996 - val_loss: 14.9105 - val_mae: 14.9105\n",
            "Epoch 10/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9500 - mae: 14.9500 - val_loss: 14.8538 - val_mae: 14.8538\n",
            "Epoch 11/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9406 - mae: 14.9406 - val_loss: 14.7163 - val_mae: 14.7163\n",
            "Epoch 12/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9358 - mae: 14.9358 - val_loss: 14.4825 - val_mae: 14.4825\n",
            "Epoch 13/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.8721 - mae: 14.8721 - val_loss: 14.7318 - val_mae: 14.7318\n",
            "Epoch 14/100\n",
            "309/309 [==============================] - 1s 3ms/step - loss: 14.9261 - mae: 14.9261 - val_loss: 14.6692 - val_mae: 14.6692\n",
            "Epoch 15/100\n",
            "309/309 [==============================] - 1s 3ms/step - loss: 14.8778 - mae: 14.8778 - val_loss: 14.5742 - val_mae: 14.5742\n",
            "Epoch 16/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9166 - mae: 14.9166 - val_loss: 14.6267 - val_mae: 14.6267\n",
            "Epoch 17/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9357 - mae: 14.9357 - val_loss: 14.5654 - val_mae: 14.5654\n",
            "Epoch 18/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9289 - mae: 14.9289 - val_loss: 14.8092 - val_mae: 14.8092\n",
            "Epoch 19/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.8979 - mae: 14.8979 - val_loss: 14.8021 - val_mae: 14.8021\n",
            "Epoch 20/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9220 - mae: 14.9220 - val_loss: 14.7070 - val_mae: 14.7070\n",
            "Epoch 21/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.9591 - mae: 14.9591 - val_loss: 14.5130 - val_mae: 14.5130\n",
            "Epoch 22/100\n",
            "309/309 [==============================] - 1s 2ms/step - loss: 14.8963 - mae: 14.8963 - val_loss: 14.5843 - val_mae: 14.5843\n",
            "99/99 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - 1s 4ms/step - loss: 227.7293 - mae: 227.7293 - val_loss: 209.7218 - val_mae: 209.7218\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 208.1978 - mae: 208.1978 - val_loss: 165.9265 - val_mae: 165.9265\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 107.8516 - mae: 107.8516 - val_loss: 29.3396 - val_mae: 29.3396\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 20.4747 - mae: 20.4747 - val_loss: 17.0760 - val_mae: 17.0760\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9357 - mae: 15.9357 - val_loss: 17.1310 - val_mae: 17.1310\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8709 - mae: 15.8709 - val_loss: 17.2579 - val_mae: 17.2579\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9452 - mae: 15.9452 - val_loss: 16.8858 - val_mae: 16.8858\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8287 - mae: 15.8287 - val_loss: 17.4100 - val_mae: 17.4100\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8328 - mae: 15.8328 - val_loss: 16.9015 - val_mae: 16.9015\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8134 - mae: 15.8134 - val_loss: 17.0666 - val_mae: 17.0666\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8546 - mae: 15.8546 - val_loss: 17.0214 - val_mae: 17.0214\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 15.8514 - mae: 15.8514 - val_loss: 16.9192 - val_mae: 16.9192\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9349 - mae: 15.9349 - val_loss: 16.8715 - val_mae: 16.8715\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8030 - mae: 15.8030 - val_loss: 16.7108 - val_mae: 16.7108\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8119 - mae: 15.8119 - val_loss: 16.9752 - val_mae: 16.9752\n",
            "Epoch 16/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.7819 - mae: 15.7819 - val_loss: 16.9841 - val_mae: 16.9841\n",
            "Epoch 17/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.7796 - mae: 15.7796 - val_loss: 17.0099 - val_mae: 17.0099\n",
            "Epoch 18/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9048 - mae: 15.9048 - val_loss: 16.9086 - val_mae: 16.9086\n",
            "Epoch 19/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 15.9103 - mae: 15.9103 - val_loss: 17.0624 - val_mae: 17.0624\n",
            "Epoch 20/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 15.8746 - mae: 15.8746 - val_loss: 16.7905 - val_mae: 16.7905\n",
            "Epoch 21/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.8830 - mae: 15.8830 - val_loss: 16.8780 - val_mae: 16.8780\n",
            "Epoch 22/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.9524 - mae: 15.9524 - val_loss: 16.8208 - val_mae: 16.8208\n",
            "Epoch 23/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 15.8519 - mae: 15.8519 - val_loss: 17.2389 - val_mae: 17.2389\n",
            "Epoch 24/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 15.9166 - mae: 15.9166 - val_loss: 16.9925 - val_mae: 16.9925\n",
            "29/29 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 88.0402 - mae: 88.0402 - val_loss: 27.6246 - val_mae: 27.6246\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 21.0764 - mae: 21.0764 - val_loss: 20.6912 - val_mae: 20.6912\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0493 - mae: 20.0493 - val_loss: 20.7551 - val_mae: 20.7551\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0586 - mae: 20.0586 - val_loss: 20.6361 - val_mae: 20.6361\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0237 - mae: 20.0237 - val_loss: 20.5200 - val_mae: 20.5200\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9898 - mae: 19.9898 - val_loss: 20.5125 - val_mae: 20.5125\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0610 - mae: 20.0610 - val_loss: 20.5214 - val_mae: 20.5214\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0366 - mae: 20.0366 - val_loss: 20.7540 - val_mae: 20.7540\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0622 - mae: 20.0622 - val_loss: 20.4533 - val_mae: 20.4533\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0680 - mae: 20.0680 - val_loss: 20.5285 - val_mae: 20.5285\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9877 - mae: 19.9877 - val_loss: 21.1694 - val_mae: 21.1694\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9914 - mae: 19.9914 - val_loss: 20.5087 - val_mae: 20.5087\n",
            "Epoch 13/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9399 - mae: 19.9399 - val_loss: 20.6301 - val_mae: 20.6301\n",
            "Epoch 14/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0207 - mae: 20.0207 - val_loss: 20.5540 - val_mae: 20.5540\n",
            "Epoch 15/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0018 - mae: 20.0018 - val_loss: 20.8746 - val_mae: 20.8746\n",
            "Epoch 16/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9933 - mae: 19.9933 - val_loss: 20.4307 - val_mae: 20.4307\n",
            "Epoch 17/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0738 - mae: 20.0738 - val_loss: 20.4707 - val_mae: 20.4707\n",
            "Epoch 18/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0352 - mae: 20.0352 - val_loss: 20.6564 - val_mae: 20.6564\n",
            "Epoch 19/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0351 - mae: 20.0351 - val_loss: 20.9017 - val_mae: 20.9017\n",
            "Epoch 20/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9925 - mae: 19.9925 - val_loss: 20.4613 - val_mae: 20.4613\n",
            "Epoch 21/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0122 - mae: 20.0122 - val_loss: 20.5339 - val_mae: 20.5339\n",
            "Epoch 22/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0085 - mae: 20.0085 - val_loss: 20.8167 - val_mae: 20.8167\n",
            "Epoch 23/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0138 - mae: 20.0138 - val_loss: 20.6968 - val_mae: 20.6968\n",
            "Epoch 24/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 19.9629 - mae: 19.9629 - val_loss: 20.4504 - val_mae: 20.4504\n",
            "Epoch 25/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0139 - mae: 20.0139 - val_loss: 20.4534 - val_mae: 20.4534\n",
            "Epoch 26/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 20.0083 - mae: 20.0083 - val_loss: 20.4746 - val_mae: 20.4746\n",
            "97/97 [==============================] - 0s 929us/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 226.8146 - mae: 226.8146 - val_loss: 226.1693 - val_mae: 226.1693\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 206.4205 - mae: 206.4205 - val_loss: 179.1713 - val_mae: 179.1713\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 111.6869 - mae: 111.6869 - val_loss: 45.0538 - val_mae: 45.0538\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 25.7930 - mae: 25.7930 - val_loss: 15.8112 - val_mae: 15.8112\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.4945 - mae: 15.4945 - val_loss: 14.0713 - val_mae: 14.0713\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.2852 - mae: 15.2852 - val_loss: 13.8792 - val_mae: 13.8792\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.2261 - mae: 15.2261 - val_loss: 13.9876 - val_mae: 13.9876\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.2999 - mae: 15.2999 - val_loss: 13.8625 - val_mae: 13.8625\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3156 - mae: 15.3156 - val_loss: 13.8353 - val_mae: 13.8353\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.2606 - mae: 15.2606 - val_loss: 14.1246 - val_mae: 14.1246\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3574 - mae: 15.3574 - val_loss: 13.9904 - val_mae: 13.9904\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.3322 - mae: 15.3322 - val_loss: 13.8443 - val_mae: 13.8443\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3035 - mae: 15.3035 - val_loss: 13.9638 - val_mae: 13.9638\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3428 - mae: 15.3428 - val_loss: 13.9301 - val_mae: 13.9301\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.1662 - mae: 15.1662 - val_loss: 14.2407 - val_mae: 14.2407\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3171 - mae: 15.3171 - val_loss: 14.0517 - val_mae: 14.0517\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 15.1613 - mae: 15.1613 - val_loss: 13.8986 - val_mae: 13.8986\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.2098 - mae: 15.2098 - val_loss: 14.0443 - val_mae: 14.0443\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 15.3180 - mae: 15.3180 - val_loss: 13.8558 - val_mae: 13.8558\n",
            "30/30 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 139.7836 - mae: 139.7836 - val_loss: 12.1311 - val_mae: 12.1311\n",
            "Epoch 2/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.3368 - mae: 8.3368 - val_loss: 7.7915 - val_mae: 7.7915\n",
            "Epoch 3/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1513 - mae: 8.1513 - val_loss: 7.9661 - val_mae: 7.9661\n",
            "Epoch 4/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1497 - mae: 8.1497 - val_loss: 8.1259 - val_mae: 8.1259\n",
            "Epoch 5/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.2245 - mae: 8.2245 - val_loss: 7.9975 - val_mae: 7.9975\n",
            "Epoch 6/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1784 - mae: 8.1784 - val_loss: 7.9843 - val_mae: 7.9843\n",
            "Epoch 7/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1564 - mae: 8.1564 - val_loss: 8.0182 - val_mae: 8.0182\n",
            "Epoch 8/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1656 - mae: 8.1656 - val_loss: 7.9737 - val_mae: 7.9737\n",
            "Epoch 9/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1899 - mae: 8.1899 - val_loss: 7.9282 - val_mae: 7.9282\n",
            "Epoch 10/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1776 - mae: 8.1776 - val_loss: 8.0574 - val_mae: 8.0574\n",
            "Epoch 11/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1820 - mae: 8.1820 - val_loss: 7.9404 - val_mae: 7.9404\n",
            "Epoch 12/100\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 8.1748 - mae: 8.1748 - val_loss: 7.9116 - val_mae: 7.9116\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 4ms/step - loss: 282.2174 - mae: 282.2174 - val_loss: 286.7330 - val_mae: 286.7330\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 266.3250 - mae: 266.3250 - val_loss: 246.1096 - val_mae: 246.1096\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 169.8211 - mae: 169.8211 - val_loss: 80.3075 - val_mae: 80.3075\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 31.0778 - mae: 31.0778 - val_loss: 11.1980 - val_mae: 11.1980\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.8109 - mae: 7.8109 - val_loss: 7.3947 - val_mae: 7.3947\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.3035 - mae: 7.3035 - val_loss: 7.1458 - val_mae: 7.1458\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.3921 - mae: 7.3921 - val_loss: 7.2115 - val_mae: 7.2115\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.2883 - mae: 7.2883 - val_loss: 6.9939 - val_mae: 6.9939\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.2951 - mae: 7.2951 - val_loss: 7.4131 - val_mae: 7.4131\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.4348 - mae: 7.4348 - val_loss: 6.7673 - val_mae: 6.7673\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.2825 - mae: 7.2825 - val_loss: 7.0507 - val_mae: 7.0507\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.3344 - mae: 7.3344 - val_loss: 7.0363 - val_mae: 7.0363\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.3550 - mae: 7.3550 - val_loss: 7.0166 - val_mae: 7.0166\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.3988 - mae: 7.3988 - val_loss: 6.7827 - val_mae: 6.7827\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.2830 - mae: 7.2830 - val_loss: 6.9524 - val_mae: 6.9524\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.2587 - mae: 7.2587 - val_loss: 6.8983 - val_mae: 6.8983\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 7.3745 - mae: 7.3745 - val_loss: 7.5168 - val_mae: 7.5168\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.3309 - mae: 7.3309 - val_loss: 7.0670 - val_mae: 7.0670\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.3005 - mae: 7.3005 - val_loss: 7.0288 - val_mae: 7.0288\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 7.3298 - mae: 7.3298 - val_loss: 6.8780 - val_mae: 6.8780\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 116.1631 - mae: 116.1631 - val_loss: 10.8542 - val_mae: 10.8542\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3920 - mae: 9.3920 - val_loss: 9.4271 - val_mae: 9.4271\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3875 - mae: 9.3875 - val_loss: 9.1814 - val_mae: 9.1814\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3117 - mae: 9.3117 - val_loss: 9.5909 - val_mae: 9.5909\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3431 - mae: 9.3431 - val_loss: 9.2907 - val_mae: 9.2907\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3116 - mae: 9.3116 - val_loss: 9.2018 - val_mae: 9.2018\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3199 - mae: 9.3199 - val_loss: 9.3075 - val_mae: 9.3075\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3686 - mae: 9.3686 - val_loss: 9.2721 - val_mae: 9.2721\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3463 - mae: 9.3463 - val_loss: 9.1675 - val_mae: 9.1675\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3676 - mae: 9.3676 - val_loss: 9.2123 - val_mae: 9.2123\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.4042 - mae: 9.4042 - val_loss: 9.1851 - val_mae: 9.1851\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.4156 - mae: 9.4156 - val_loss: 9.1692 - val_mae: 9.1692\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3370 - mae: 9.3370 - val_loss: 9.2668 - val_mae: 9.2668\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3103 - mae: 9.3103 - val_loss: 9.1931 - val_mae: 9.1931\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3167 - mae: 9.3167 - val_loss: 9.3930 - val_mae: 9.3930\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3415 - mae: 9.3415 - val_loss: 9.1593 - val_mae: 9.1593\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3587 - mae: 9.3587 - val_loss: 9.2160 - val_mae: 9.2160\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3622 - mae: 9.3622 - val_loss: 9.3395 - val_mae: 9.3395\n",
            "Epoch 19/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3794 - mae: 9.3794 - val_loss: 9.1536 - val_mae: 9.1536\n",
            "Epoch 20/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3373 - mae: 9.3373 - val_loss: 9.5234 - val_mae: 9.5234\n",
            "Epoch 21/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.2953 - mae: 9.2953 - val_loss: 9.3058 - val_mae: 9.3058\n",
            "Epoch 22/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3207 - mae: 9.3207 - val_loss: 9.1615 - val_mae: 9.1615\n",
            "Epoch 23/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3633 - mae: 9.3633 - val_loss: 9.2590 - val_mae: 9.2590\n",
            "Epoch 24/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3141 - mae: 9.3141 - val_loss: 9.2758 - val_mae: 9.2758\n",
            "Epoch 25/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3158 - mae: 9.3158 - val_loss: 9.4740 - val_mae: 9.4740\n",
            "Epoch 26/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3514 - mae: 9.3514 - val_loss: 9.2408 - val_mae: 9.2408\n",
            "Epoch 27/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3329 - mae: 9.3329 - val_loss: 9.1796 - val_mae: 9.1796\n",
            "Epoch 28/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3432 - mae: 9.3432 - val_loss: 9.2190 - val_mae: 9.2190\n",
            "Epoch 29/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 9.3858 - mae: 9.3858 - val_loss: 9.2677 - val_mae: 9.2677\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 253.4573 - mae: 253.4573 - val_loss: 251.6121 - val_mae: 251.6121\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 233.5245 - mae: 233.5245 - val_loss: 204.8307 - val_mae: 204.8307\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 136.5521 - mae: 136.5521 - val_loss: 54.1324 - val_mae: 54.1324\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 22.5016 - mae: 22.5016 - val_loss: 8.5422 - val_mae: 8.5422\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1984 - mae: 8.1984 - val_loss: 8.5475 - val_mae: 8.5475\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2460 - mae: 8.2460 - val_loss: 8.4944 - val_mae: 8.4944\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2306 - mae: 8.2306 - val_loss: 8.7662 - val_mae: 8.7662\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2719 - mae: 8.2719 - val_loss: 8.7379 - val_mae: 8.7379\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2544 - mae: 8.2544 - val_loss: 8.6815 - val_mae: 8.6815\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1211 - mae: 8.1211 - val_loss: 8.3991 - val_mae: 8.3991\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1313 - mae: 8.1313 - val_loss: 8.4350 - val_mae: 8.4350\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2248 - mae: 8.2248 - val_loss: 9.2682 - val_mae: 9.2682\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2162 - mae: 8.2162 - val_loss: 8.6873 - val_mae: 8.6873\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1883 - mae: 8.1883 - val_loss: 8.4615 - val_mae: 8.4615\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1835 - mae: 8.1835 - val_loss: 8.6186 - val_mae: 8.6186\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2186 - mae: 8.2186 - val_loss: 8.6674 - val_mae: 8.6674\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1479 - mae: 8.1479 - val_loss: 8.3941 - val_mae: 8.3941\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.0653 - mae: 8.0653 - val_loss: 8.7446 - val_mae: 8.7446\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 8.1736 - mae: 8.1736 - val_loss: 8.5265 - val_mae: 8.5265\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.0761 - mae: 8.0761 - val_loss: 8.4764 - val_mae: 8.4764\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2729 - mae: 8.2729 - val_loss: 8.7317 - val_mae: 8.7317\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1139 - mae: 8.1139 - val_loss: 8.3225 - val_mae: 8.3225\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2131 - mae: 8.2131 - val_loss: 8.9248 - val_mae: 8.9248\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1036 - mae: 8.1036 - val_loss: 8.3274 - val_mae: 8.3274\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2483 - mae: 8.2483 - val_loss: 8.3387 - val_mae: 8.3387\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1297 - mae: 8.1297 - val_loss: 8.5544 - val_mae: 8.5544\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1365 - mae: 8.1365 - val_loss: 8.5340 - val_mae: 8.5340\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1856 - mae: 8.1856 - val_loss: 8.3935 - val_mae: 8.3935\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2763 - mae: 8.2763 - val_loss: 8.7730 - val_mae: 8.7730\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1212 - mae: 8.1212 - val_loss: 8.4610 - val_mae: 8.4610\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2189 - mae: 8.2189 - val_loss: 8.3308 - val_mae: 8.3308\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1705 - mae: 8.1705 - val_loss: 8.2889 - val_mae: 8.2889\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1695 - mae: 8.1695 - val_loss: 8.7876 - val_mae: 8.7876\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1710 - mae: 8.1710 - val_loss: 8.6173 - val_mae: 8.6173\n",
            "Epoch 35/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1136 - mae: 8.1136 - val_loss: 8.4365 - val_mae: 8.4365\n",
            "Epoch 36/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1946 - mae: 8.1946 - val_loss: 8.3159 - val_mae: 8.3159\n",
            "Epoch 37/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.0917 - mae: 8.0917 - val_loss: 8.6099 - val_mae: 8.6099\n",
            "Epoch 38/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2408 - mae: 8.2408 - val_loss: 8.8171 - val_mae: 8.8171\n",
            "Epoch 39/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1465 - mae: 8.1465 - val_loss: 8.6738 - val_mae: 8.6738\n",
            "Epoch 40/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1114 - mae: 8.1114 - val_loss: 8.5877 - val_mae: 8.5877\n",
            "Epoch 41/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.1377 - mae: 8.1377 - val_loss: 8.4319 - val_mae: 8.4319\n",
            "Epoch 42/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 8.2059 - mae: 8.2059 - val_loss: 8.9666 - val_mae: 8.9666\n",
            "31/31 [==============================] - 0s 966us/step\n",
            "Epoch 1/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 114.7329 - mae: 114.7329 - val_loss: 17.4293 - val_mae: 17.4293\n",
            "Epoch 2/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.2128 - mae: 13.2128 - val_loss: 13.0170 - val_mae: 13.0170\n",
            "Epoch 3/100\n",
            "307/307 [==============================] - 1s 3ms/step - loss: 13.0178 - mae: 13.0178 - val_loss: 12.9657 - val_mae: 12.9657\n",
            "Epoch 4/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0824 - mae: 13.0824 - val_loss: 13.1022 - val_mae: 13.1022\n",
            "Epoch 5/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0612 - mae: 13.0612 - val_loss: 13.0275 - val_mae: 13.0275\n",
            "Epoch 6/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0155 - mae: 13.0155 - val_loss: 12.9888 - val_mae: 12.9888\n",
            "Epoch 7/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0066 - mae: 13.0066 - val_loss: 12.8859 - val_mae: 12.8859\n",
            "Epoch 8/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 12.9872 - mae: 12.9872 - val_loss: 13.2967 - val_mae: 13.2967\n",
            "Epoch 9/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0605 - mae: 13.0605 - val_loss: 12.9947 - val_mae: 12.9947\n",
            "Epoch 10/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 12.9888 - mae: 12.9888 - val_loss: 13.0058 - val_mae: 13.0058\n",
            "Epoch 11/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0372 - mae: 13.0372 - val_loss: 12.9806 - val_mae: 12.9806\n",
            "Epoch 12/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0585 - mae: 13.0585 - val_loss: 13.0247 - val_mae: 13.0247\n",
            "Epoch 13/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0276 - mae: 13.0276 - val_loss: 12.9156 - val_mae: 12.9156\n",
            "Epoch 14/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 12.9855 - mae: 12.9855 - val_loss: 13.1201 - val_mae: 13.1201\n",
            "Epoch 15/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 12.9602 - mae: 12.9602 - val_loss: 13.0843 - val_mae: 13.0843\n",
            "Epoch 16/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0066 - mae: 13.0066 - val_loss: 13.0257 - val_mae: 13.0257\n",
            "Epoch 17/100\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 13.0379 - mae: 13.0379 - val_loss: 12.9872 - val_mae: 12.9872\n",
            "101/101 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "95/95 [==============================] - 1s 3ms/step - loss: 250.3777 - mae: 250.3777 - val_loss: 237.5106 - val_mae: 237.5106\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 228.5621 - mae: 228.5621 - val_loss: 188.0408 - val_mae: 188.0408\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 120.1913 - mae: 120.1913 - val_loss: 30.4098 - val_mae: 30.4098\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 15.4783 - mae: 15.4783 - val_loss: 10.5502 - val_mae: 10.5502\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.4835 - mae: 11.4835 - val_loss: 10.6930 - val_mae: 10.6930\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.4471 - mae: 11.4471 - val_loss: 10.6147 - val_mae: 10.6147\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.3172 - mae: 11.3172 - val_loss: 10.4531 - val_mae: 10.4531\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.3713 - mae: 11.3713 - val_loss: 10.3668 - val_mae: 10.3668\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.2675 - mae: 11.2675 - val_loss: 10.4725 - val_mae: 10.4725\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - 0s 4ms/step - loss: 11.4385 - mae: 11.4385 - val_loss: 10.4273 - val_mae: 10.4273\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.3842 - mae: 11.3842 - val_loss: 10.4974 - val_mae: 10.4974\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.4236 - mae: 11.4236 - val_loss: 10.4399 - val_mae: 10.4399\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.3829 - mae: 11.3829 - val_loss: 10.5337 - val_mae: 10.5337\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.3232 - mae: 11.3232 - val_loss: 10.5361 - val_mae: 10.5361\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 11.3046 - mae: 11.3046 - val_loss: 10.4374 - val_mae: 10.4374\n",
            "Epoch 16/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.2198 - mae: 11.2198 - val_loss: 10.4658 - val_mae: 10.4658\n",
            "Epoch 17/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.2332 - mae: 11.2332 - val_loss: 10.3890 - val_mae: 10.3890\n",
            "Epoch 18/100\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 11.3929 - mae: 11.3929 - val_loss: 10.4696 - val_mae: 10.4696\n",
            "29/29 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 108.3219 - mae: 108.3219 - val_loss: 19.6747 - val_mae: 19.6747\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 4ms/step - loss: 7.0001 - mae: 7.0001 - val_loss: 5.6743 - val_mae: 5.6743\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5472 - mae: 5.5472 - val_loss: 5.4572 - val_mae: 5.4572\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5606 - mae: 5.5606 - val_loss: 5.5357 - val_mae: 5.5357\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5732 - mae: 5.5732 - val_loss: 5.3078 - val_mae: 5.3078\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.4740 - mae: 5.4740 - val_loss: 5.4856 - val_mae: 5.4856\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5124 - mae: 5.5124 - val_loss: 5.7407 - val_mae: 5.7407\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5712 - mae: 5.5712 - val_loss: 5.4571 - val_mae: 5.4571\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5405 - mae: 5.5405 - val_loss: 5.4804 - val_mae: 5.4804\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5813 - mae: 5.5813 - val_loss: 5.4712 - val_mae: 5.4712\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.5724 - mae: 5.5724 - val_loss: 5.3721 - val_mae: 5.3721\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 5.6019 - mae: 5.6019 - val_loss: 5.4616 - val_mae: 5.4616\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 5.5222 - mae: 5.5222 - val_loss: 5.6078 - val_mae: 5.6078\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 5.5916 - mae: 5.5916 - val_loss: 5.3737 - val_mae: 5.3737\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 3ms/step - loss: 5.5285 - mae: 5.5285 - val_loss: 5.4301 - val_mae: 5.4301\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 241.9302 - mae: 241.9302 - val_loss: 235.7749 - val_mae: 235.7749\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 218.6703 - mae: 218.6703 - val_loss: 187.6016 - val_mae: 187.6016\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 131.9627 - mae: 131.9627 - val_loss: 59.1899 - val_mae: 59.1899\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 25.2192 - mae: 25.2192 - val_loss: 5.8114 - val_mae: 5.8114\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.2119 - mae: 5.2119 - val_loss: 4.9750 - val_mae: 4.9750\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.0463 - mae: 5.0463 - val_loss: 4.9682 - val_mae: 4.9682\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.2235 - mae: 5.2235 - val_loss: 5.2689 - val_mae: 5.2689\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1828 - mae: 5.1828 - val_loss: 5.1479 - val_mae: 5.1479\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.0702 - mae: 5.0702 - val_loss: 5.0802 - val_mae: 5.0802\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.4201 - mae: 5.4201 - val_loss: 5.0282 - val_mae: 5.0282\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.1213 - mae: 5.1213 - val_loss: 4.9304 - val_mae: 4.9304\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.1013 - mae: 5.1013 - val_loss: 4.9059 - val_mae: 4.9059\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.0607 - mae: 5.0607 - val_loss: 4.9727 - val_mae: 4.9727\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1017 - mae: 5.1017 - val_loss: 4.8956 - val_mae: 4.8956\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1556 - mae: 5.1556 - val_loss: 5.2532 - val_mae: 5.2532\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1951 - mae: 5.1951 - val_loss: 5.0529 - val_mae: 5.0529\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1324 - mae: 5.1324 - val_loss: 5.1706 - val_mae: 5.1706\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.1329 - mae: 5.1329 - val_loss: 4.9321 - val_mae: 4.9321\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.2986 - mae: 5.2986 - val_loss: 4.9802 - val_mae: 4.9802\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.0585 - mae: 5.0585 - val_loss: 5.0270 - val_mae: 5.0270\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.1671 - mae: 5.1671 - val_loss: 5.1360 - val_mae: 5.1360\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.2828 - mae: 5.2828 - val_loss: 5.3196 - val_mae: 5.3196\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 5.3099 - mae: 5.3099 - val_loss: 4.9087 - val_mae: 4.9087\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.0986 - mae: 5.0986 - val_loss: 5.4099 - val_mae: 5.4099\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Overall R^2 for Neural Network: 0.9877771214650237\n",
            "Overall MAE for Neural Network: 0\n",
            "    stop_sequence day_type       R^2        MAE\n",
            "0               1  weekday  0.893328  27.462216\n",
            "1               1  weekend  0.960194  26.420752\n",
            "2               5  weekday  0.980117  14.268920\n",
            "3               5  weekend  0.991467  11.818354\n",
            "4               8  weekday  0.978241  16.085863\n",
            "5               8  weekend  0.992957  12.492429\n",
            "6              12  weekday  0.990833  11.488217\n",
            "7              12  weekend  0.995526  10.198349\n",
            "8              14  weekday  0.986459  14.784705\n",
            "9              14  weekend  0.991495  15.729365\n",
            "10             16  weekday  0.974181  19.446008\n",
            "11             16  weekend  0.992211  14.899843\n",
            "12             19  weekday  0.995405   7.914564\n",
            "13             19  weekend  0.997834   6.875038\n",
            "14             20  weekday  0.994388   9.338775\n",
            "15             20  weekend  0.997998   7.221683\n",
            "16             21  weekday  0.991257  12.949567\n",
            "17             21  weekend  0.996290  10.463418\n",
            "18             23  weekday  0.998181   5.390933\n",
            "19             23  weekend  0.999006   4.918394\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate_nn(df):\n",
        "    # Split the dataset into training and test sets\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)  # Use df, not df_selected_stops\n",
        "\n",
        "    if df_train.empty or df_test.empty:\n",
        "        print(\"Training or testing dataset is empty.\")\n",
        "        return None  # Return early if the dataset is empty\n",
        "\n",
        "    sc = StandardScaler()\n",
        "\n",
        "    y_mean = df_test['arrival_delay'].mean()\n",
        "    overall_ssr = 0  # Initialize overall_ssr\n",
        "    overall_tss = 0  # Initialize overall_tss\n",
        "    overall_mae = 0  # Initialize overall MAE\n",
        "\n",
        "    results = {\n",
        "        'stop_sequence': [],\n",
        "        'day_type': [],\n",
        "        'R^2': [],\n",
        "        'MAE': []\n",
        "    }\n",
        "\n",
        "    for stop_seq in df['stop_sequence'].unique():\n",
        "        for day_type in ['weekday', 'weekend']:\n",
        "            # Filter data\n",
        "            df_train_subset = df_train[(df_train['stop_sequence'] == stop_seq) & (df_train['day_of_week'] == day_type)]\n",
        "            df_test_subset = df_test[(df_test['stop_sequence'] == stop_seq) & (df_test['day_of_week'] == day_type)]\n",
        "\n",
        "            # Prepare data\n",
        "            x_train = df_train_subset.drop(['arrival_delay', 'day_of_week'], axis=1)\n",
        "            y_train = df_train_subset['arrival_delay']\n",
        "\n",
        "            x_test = df_test_subset.drop(['arrival_delay', 'day_of_week'], axis=1)\n",
        "            y_test = df_test_subset['arrival_delay']\n",
        "\n",
        "            # Normalize the input features\n",
        "            x_train = sc.fit_transform(x_train)\n",
        "            x_test = sc.transform(x_test)\n",
        "\n",
        "            # Neural network model\n",
        "            model = Sequential()\n",
        "            model.add(Dense(32, activation='linear', input_dim=x_train.shape[1]))\n",
        "            model.add(Dropout(0.001))\n",
        "            model.add(Dense(64, activation='linear'))\n",
        "            model.add(Dense(1))\n",
        "\n",
        "            model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "            model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "            y_pred = model.predict(x_test).flatten()\n",
        "            current_r2 = r2_score(y_test, y_pred)\n",
        "            current_mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "            residuals = y_test - y_pred\n",
        "            ssr = sum(residuals**2)\n",
        "            overall_ssr += ssr\n",
        "            tss = sum((y_test - y_mean)**2)\n",
        "            overall_tss += tss\n",
        "\n",
        "            results['stop_sequence'].append(stop_seq)\n",
        "            results['day_type'].append(day_type)\n",
        "            results['R^2'].append(current_r2)\n",
        "            results['MAE'].append(current_mae)\n",
        "\n",
        "    overall_r2 = 1 - (overall_ssr / overall_tss)\n",
        "\n",
        "    print(f'Overall R^2 for Neural Network: {overall_r2}')\n",
        "    print(f'Overall MAE for Neural Network: {overall_mae}')\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "results_df = train_and_evaluate_nn(df2)\n",
        "if results_df is not None:\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kyMzP8TOVD8I",
      "metadata": {
        "id": "kyMzP8TOVD8I"
      },
      "source": [
        "### Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "Len0ocwcVGjx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Len0ocwcVGjx",
        "outputId": "f880b507-f5df-4b83-dd49-797e918dd4c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall R^2: 0.9887131991644134\n",
            "Overall MAE: 13.802761161242874\n",
            "    stop_sequence day_type       R^2        MAE\n",
            "0               1  weekday  0.905814  28.669563\n",
            "1               1  weekend  0.962507  27.498654\n",
            "2               5  weekday  0.980929  14.488254\n",
            "3               5  weekend  0.991648  11.942039\n",
            "4               8  weekday  0.979634  16.346677\n",
            "5               8  weekend  0.993247  12.418298\n",
            "6              12  weekday  0.991910  12.081522\n",
            "7              12  weekend  0.995837  10.709357\n",
            "8              14  weekday  0.987454  14.903614\n",
            "9              14  weekend  0.991567  15.987618\n",
            "10             16  weekday  0.975778  20.084594\n",
            "11             16  weekend  0.992523  14.931404\n",
            "12             19  weekday  0.995635   8.189313\n",
            "13             19  weekend  0.997917   7.042332\n",
            "14             20  weekday  0.994866   9.377015\n",
            "15             20  weekend  0.998005   7.479137\n",
            "16             21  weekday  0.991709  13.200955\n",
            "17             21  weekend  0.996486  10.622142\n",
            "18             23  weekday  0.998244   5.484677\n",
            "19             23  weekend  0.999057   4.796681\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "def train_and_evaluate(df):\n",
        "    # Split the entire dataset into training and test sets\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    y_mean = df_test['arrival_delay'].mean()\n",
        "\n",
        "    results = {\n",
        "        'stop_sequence': [],\n",
        "        'day_type': [],\n",
        "        'R^2': [],\n",
        "        'MAE': []\n",
        "    }\n",
        "\n",
        "    weighted_mae_sum = 0\n",
        "    total_samples = 0\n",
        "    overall_ssr = 0\n",
        "    overall_tss = 0\n",
        "\n",
        "    for stop_seq in df['stop_sequence'].unique():\n",
        "        for day_type in ['weekday', 'weekend']:\n",
        "            # Filter data by stop sequence and day type\n",
        "            df_train_subset = df_train[(df_train['stop_sequence'] == stop_seq) & (df_train['day_of_week'] == day_type)]\n",
        "            df_test_subset = df_test[(df_test['stop_sequence'] == stop_seq) & (df_test['day_of_week'] == day_type)]\n",
        "\n",
        "            # Train model with RFECV\n",
        "            x_train = df_train_subset.drop(['arrival_delay', 'day_of_week'], axis=1)\n",
        "            y_train = df_train_subset['arrival_delay']\n",
        "\n",
        "            model = LinearRegression()\n",
        "            selector = RFECV(estimator=model, step=1, cv=KFold(5))\n",
        "            selector = selector.fit(x_train, y_train)\n",
        "\n",
        "            # Fit model with selected features\n",
        "            model.fit(x_train.iloc[:, selector.support_], y_train)\n",
        "\n",
        "            # Evaluate the model\n",
        "            x_test = df_test_subset.drop(['arrival_delay', 'day_of_week'], axis=1).iloc[:, selector.support_]\n",
        "            y_test = df_test_subset['arrival_delay']\n",
        "\n",
        "            y_pred = model.predict(x_test)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            sample_count = len(y_test)\n",
        "            weighted_mae_sum += mae * sample_count\n",
        "            total_samples += sample_count\n",
        "\n",
        "            residuals = y_test - y_pred\n",
        "            ssr = sum(residuals**2)\n",
        "            overall_ssr += ssr\n",
        "            tss = sum((y_test - y_mean)**2)\n",
        "            overall_tss += tss\n",
        "\n",
        "            results['stop_sequence'].append(stop_seq)\n",
        "            results['day_type'].append(day_type)\n",
        "            results['R^2'].append(r2)\n",
        "            results['MAE'].append(mae)\n",
        "\n",
        "\n",
        "    overall_r2 = 1 - (overall_ssr / overall_tss)\n",
        "    overall_mae = weighted_mae_sum / total_samples\n",
        "\n",
        "    print(f'Overall R^2: {overall_r2}')\n",
        "    print(f'Overall MAE: {overall_mae}')\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "results_df = train_and_evaluate(df2)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR VS NN"
      ],
      "metadata": {
        "id": "ITwytOSO2wyv"
      },
      "id": "ITwytOSO2wyv"
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHlKEVJU4H81",
        "outputId": "b80aded3-b5cb-4286-d9e1-549c9250476e"
      },
      "id": "cHlKEVJU4H81",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 201801 entries, 0 to 545098\n",
            "Data columns (total 12 columns):\n",
            " #   Column                            Non-Null Count   Dtype \n",
            "---  ------                            --------------   ----- \n",
            " 0   stop_sequence                     201801 non-null  int64 \n",
            " 1   arrival_delay                     201801 non-null  int64 \n",
            " 2   dwell_time                        201801 non-null  int64 \n",
            " 3   travel_time_for_previous_section  201801 non-null  int64 \n",
            " 4   scheduled_travel_time             201801 non-null  int64 \n",
            " 5   upstream_stop_delay               201801 non-null  int64 \n",
            " 6   origin_delay                      201801 non-null  int64 \n",
            " 7   previous_bus_delay                201801 non-null  int64 \n",
            " 8   previous_trip_travel_time         201801 non-null  int64 \n",
            " 9   traffic_condition                 201801 non-null  int64 \n",
            " 10  recurrent_delay                   201801 non-null  int64 \n",
            " 11  day_of_week                       201801 non-null  object\n",
            "dtypes: int64(11), object(1)\n",
            "memory usage: 20.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def train_evaluate_best_model(df, n_folds=5):\n",
        "\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    results = {\n",
        "        'stop_sequence': [],\n",
        "        'day_type': [],\n",
        "        'best_model': [],\n",
        "        'R^2': [],\n",
        "        'MAE': []\n",
        "    }\n",
        "\n",
        "    overall_r2_values = []\n",
        "    overall_sample_weights = []\n",
        "\n",
        "    for stop_seq in df['stop_sequence'].unique():\n",
        "        for day_type in ['weekday', 'weekend']:\n",
        "            df_train_subset = df_train[(df_train['stop_sequence'] == stop_seq) & (df_train['day_of_week'] == day_type)]\n",
        "            df_test_subset = df_test[(df_test['stop_sequence'] == stop_seq) & (df_test['day_of_week'] == day_type)]\n",
        "\n",
        "            drop_columns = ['arrival_delay', 'day_of_week']\n",
        "            x_train = df_train_subset.drop(columns=drop_columns, errors='ignore').astype('float32')\n",
        "            y_train = df_train_subset['arrival_delay'].astype('float32')\n",
        "            x_test = df_test_subset.drop(columns=drop_columns, errors='ignore').astype('float32')\n",
        "            y_test = df_test_subset['arrival_delay'].astype('float32')\n",
        "\n",
        "            r2_scores_nn = []\n",
        "            r2_scores_lr = []\n",
        "            maes_nn = []\n",
        "            maes_lr = []\n",
        "\n",
        "            # K-fold CV\n",
        "            kf = KFold(n_splits=n_folds)\n",
        "            for train_index, val_index in kf.split(x_train):\n",
        "                x_train_fold, x_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
        "                y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "                # Train and evaluate NN\n",
        "                model_nn = Sequential([\n",
        "                    Dense(32, activation='relu', input_dim=x_train_fold.shape[1]),\n",
        "                    Dropout(0.001),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "                model_nn.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "                model_nn.fit(x_train_fold, y_train_fold, validation_data=(x_val_fold, y_val_fold), epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
        "                y_pred_nn = model_nn.predict(x_val_fold).flatten()\n",
        "                r2_scores_nn.append(r2_score(y_val_fold, y_pred_nn))\n",
        "                maes_nn.append(mean_absolute_error(y_val_fold, y_pred_nn))\n",
        "\n",
        "                # Train and evaluate LR\n",
        "                model_lr = LinearRegression().fit(x_train_fold, y_train_fold)\n",
        "                y_pred_lr = model_lr.predict(x_val_fold)\n",
        "                r2_scores_lr.append(r2_score(y_val_fold, y_pred_lr))\n",
        "                maes_lr.append(mean_absolute_error(y_val_fold, y_pred_lr))\n",
        "\n",
        "            # Average scores\n",
        "            avg_r2_nn = np.mean(r2_scores_nn)\n",
        "            avg_r2_lr = np.mean(r2_scores_lr)\n",
        "            avg_mae_nn = np.mean(maes_nn)\n",
        "            avg_mae_lr = np.mean(maes_lr)\n",
        "\n",
        "            # Determine best model\n",
        "            if avg_r2_nn > avg_r2_lr:\n",
        "                best_model = \"NN\"\n",
        "                best_r2 = avg_r2_nn\n",
        "                best_mae = avg_mae_nn\n",
        "            else:\n",
        "                best_model = \"LR\"\n",
        "                best_r2 = avg_r2_lr\n",
        "                best_mae = avg_mae_lr\n",
        "\n",
        "            # Save models\n",
        "            model_dir = \"models\"\n",
        "            if not os.path.exists(model_dir):\n",
        "                os.makedirs(model_dir)\n",
        "            if best_model == \"NN\":\n",
        "                model_nn.save(os.path.join(model_dir, f'nn_model_{stop_seq}_{day_type}.h5'))\n",
        "            else:\n",
        "                joblib.dump(model_lr, os.path.join(model_dir, f'lr_model_{stop_seq}_{day_type}.pkl'))\n",
        "\n",
        "            overall_r2_values.append(best_r2)\n",
        "            overall_sample_weights.append(len(df_test_subset))\n",
        "            results['stop_sequence'].append(stop_seq)\n",
        "            results['day_type'].append(day_type)\n",
        "            results['best_model'].append(best_model)\n",
        "            results['R^2'].append(best_r2)\n",
        "            results['MAE'].append(best_mae)\n",
        "\n",
        "    overall_r2 = np.average(overall_r2_values, weights=overall_sample_weights)\n",
        "    print(f\"Overall R^2: {overall_r2:.2f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# predict_new_data function remains mostly the same.\n",
        "def predict_new_data(df2):\n",
        "    all_groups = []  # Placeholder for updated groups\n",
        "\n",
        "    # Group the dataframe by 'stop_sequence' and 'day_of_week'\n",
        "    grouped = df.groupby(['stop_sequence', 'day_of_week'])\n",
        "\n",
        "    for (stop_seq, day_type), group in grouped:\n",
        "        nn_model_path = os.path.join(\"models\", f'nn_model_{stop_seq}_{day_type}.h5')\n",
        "        lr_model_path = os.path.join(\"models\", f'lr_model_{stop_seq}_{day_type}.pkl')\n",
        "\n",
        "        X = group.drop(columns=['arrival_delay', 'day_of_week'], errors='ignore').astype('float32')\n",
        "\n",
        "        if os.path.exists(nn_model_path):\n",
        "            model = load_model(nn_model_path)\n",
        "            group['predicted_delay'] = model.predict(X).flatten()\n",
        "\n",
        "        elif os.path.exists(lr_model_path):\n",
        "            model = joblib.load(lr_model_path)\n",
        "            group['predicted_delay'] = model.predict(X)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"No saved model found for stop_sequence: {stop_seq} and day_type: {day_type}\")\n",
        "\n",
        "        all_groups.append(group)\n",
        "\n",
        "    # Concatenate all updated groups to get the final dataframe\n",
        "    result_df = pd.concat(all_groups)\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "m84H9YMEJskE"
      },
      "id": "m84H9YMEJskE",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = train_evaluate_best_model(df2)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtW7xYAKkeV",
        "outputId": "242c3792-bd5a-4927-cdc1-796a702e829d"
      },
      "id": "CgtW7xYAKkeV",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 0s 953us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 995us/step\n",
            "78/78 [==============================] - 0s 977us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 963us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 914us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 930us/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 986us/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 933us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 972us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 975us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 998us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 952us/step\n",
            "78/78 [==============================] - 0s 983us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 984us/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1000us/step\n",
            "78/78 [==============================] - 0s 957us/step\n",
            "78/78 [==============================] - 0s 943us/step\n",
            "78/78 [==============================] - 0s 924us/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 931us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 889us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "77/77 [==============================] - 0s 927us/step\n",
            "77/77 [==============================] - 0s 1ms/step\n",
            "77/77 [==============================] - 0s 1ms/step\n",
            "77/77 [==============================] - 0s 924us/step\n",
            "77/77 [==============================] - 0s 954us/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 959us/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "78/78 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "Overall R^2: 0.98\n",
            "    stop_sequence day_type best_model       R^2        MAE\n",
            "0               1  weekday         NN  0.901781  27.110336\n",
            "1               1  weekend         LR  0.954502  26.964746\n",
            "2               5  weekday         LR  0.977974  14.600647\n",
            "3               5  weekend         LR  0.992198  11.490544\n",
            "4               8  weekday         LR  0.977179  16.290564\n",
            "5               8  weekend         LR  0.992663  12.721334\n",
            "6              12  weekday         LR  0.990531  12.285365\n",
            "7              12  weekend         LR  0.996006  10.378367\n",
            "8              14  weekday         LR  0.986910  14.954063\n",
            "9              14  weekend         LR  0.991185  16.074905\n",
            "10             16  weekday         LR  0.976267  20.499210\n",
            "11             16  weekend         LR  0.992088  14.911676\n",
            "12             19  weekday         LR  0.995265   8.105009\n",
            "13             19  weekend         LR  0.997908   7.037652\n",
            "14             20  weekday         LR  0.994972   9.331365\n",
            "15             20  weekend         LR  0.997567   8.168992\n",
            "16             21  weekday         LR  0.992247  13.018295\n",
            "17             21  weekend         LR  0.996280  11.079016\n",
            "18             23  weekday         LR  0.998408   5.372571\n",
            "19             23  weekend         LR  0.999163   4.752423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for stop_seq in df2['stop_sequence'].unique():\n",
        "    for day_type in ['weekday', 'weekend']:\n",
        "        subset = df2[(df2['stop_sequence'] == stop_seq) & (df2['day_of_week'] == day_type)]\n",
        "        print(f\"stop_seq: {stop_seq}, day_type: {day_type}, size: {len(subset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sBKBTCoKwcP",
        "outputId": "a79c981f-7b2d-42f1-bb3e-14719e88be89"
      },
      "id": "-sBKBTCoKwcP",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop_seq: 1, day_type: weekday, size: 15480\n",
            "stop_seq: 1, day_type: weekend, size: 4702\n",
            "stop_seq: 5, day_type: weekday, size: 15480\n",
            "stop_seq: 5, day_type: weekend, size: 4702\n",
            "stop_seq: 8, day_type: weekday, size: 15479\n",
            "stop_seq: 8, day_type: weekend, size: 4702\n",
            "stop_seq: 12, day_type: weekday, size: 15480\n",
            "stop_seq: 12, day_type: weekend, size: 4702\n",
            "stop_seq: 14, day_type: weekday, size: 15480\n",
            "stop_seq: 14, day_type: weekend, size: 4702\n",
            "stop_seq: 16, day_type: weekday, size: 15480\n",
            "stop_seq: 16, day_type: weekend, size: 4702\n",
            "stop_seq: 19, day_type: weekday, size: 15473\n",
            "stop_seq: 19, day_type: weekend, size: 4702\n",
            "stop_seq: 20, day_type: weekday, size: 15477\n",
            "stop_seq: 20, day_type: weekend, size: 4702\n",
            "stop_seq: 21, day_type: weekday, size: 15476\n",
            "stop_seq: 21, day_type: weekend, size: 4702\n",
            "stop_seq: 23, day_type: weekday, size: 15476\n",
            "stop_seq: 23, day_type: weekend, size: 4702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the trained models"
      ],
      "metadata": {
        "id": "WX-4kwr-K6DS"
      },
      "id": "WX-4kwr-K6DS"
    },
    {
      "cell_type": "code",
      "source": [
        "pred = predict_new_data(df2)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "aibH2lWYK-MD",
        "outputId": "77bd69e2-47a9-4f3c-f314-b50ff62112f9"
      },
      "id": "aibH2lWYK-MD",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-68c954dae049>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-77c4cc6ed213>\u001b[0m in \u001b[0;36mpredict_new_data\u001b[0;34m(df2)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Group the dataframe by 'stop_sequence' and 'day_of_week'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stop_sequence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day_of_week'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstop_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8400\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8402\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   8403\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8404\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    966\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'day_of_week'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.head()"
      ],
      "metadata": {
        "id": "7pv6WkDQLCAR"
      },
      "id": "7pv6WkDQLCAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the actual and predicted values from the DataFrame\n",
        "actual_values = pred[\"arrival_delay\"].values\n",
        "predicted_values = pred[\"predicGQHW§EÖted_delay\"].values\n",
        "\n",
        "# Computing R^2\n",
        "r2 = r2_score(actual_values, predicted_values)\n",
        "\n",
        "# Computing MAE\n",
        "mae = mean_absolute_error(actual_values, predicted_values)\n",
        "\n",
        "print(f\"R^2: {r2:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "id": "snFq_SAhLD2Z"
      },
      "id": "snFq_SAhLD2Z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}